---
title: "vLLM(1)-LLM 추론과 서비스의 효율성 향상"
date: 2025-03-11
tags: [LLM, vLLM, OpenSource, 오픈소스]
typora-root-url: ../
toc: true
categories: [vLLM, OpenSource]
---

LLaMA, Mistral, GPT와 같은 대형 언어 모델(LLM)은 인간과 유사한 텍스트 생성, 챗봇 구동, 코드 생성 및 콘텐츠 제작 보조 등 다양한 산업에서 혁신을 일으키고 있다. 하지만 이러한 모델을 실제 환경에 배포하는 데는 막대한 연산 및 메모리 자원이 필요하다는 도전 과제가 따른다는 것을 수없이 말했다. 

이 문제를 해결하기 위해 등장한 것이 **vLLM**이다. vLLM은 LLM 추론 및 서비스 속도를 빠르고 효율적으로 만들기 위해 개발된 오픈소스 라이브러리이다. 이 노트에서는 vLLM의 핵심 기능을 설명하겠다. 



## **1. vLLM이란?**

* vLLM(Virtual Large Language Model)은 UC 버클리에서 개발된 오픈소스 라이브러리로, LLM 추론과 서비스의 효율성을 향상시키기 위해 만들어졌음
* vLLM의 장점 
  * 높은 메모리 사용량
  * 느린 추론 속도
  * 비효율적인 리소스 활용
* 특징
  * PagedAttention, Continuous Batching, 양자화 지원 등의 혁신 기술을 통해, vLLM은 Hugging Face Transformers와 같은 기존 프레임워크보다 **최대 24배 더 높은 처리량**을 보여주며, 메모리 낭비도 크게 줄임



## **2. vLLM이 중요한 이유**

* LLaMA-13B 모델은 16비트 정밀도(FP16)에서 약 **26GB의 메모리**를 요구하는 데, 기존 시스템의 한계

  * **메모리 낭비**: Attention Key-Value(KV) 캐시를 연속된 메모리에 저장함으로써 조각화(fragmentation) 및 낭비 발생
  * **느린 응답 시간**: 비효율적인 배치 처리로 인한 지연
  * **높은 비용**: 멀티 GPU 환경에서 모델을 실행해야 함

* 문제 해결

  * **대화형 AI, 코드 보조, 대규모 텍스트 생성** 등의 실제 응용 사례에 적합한 솔루션
  * Hugging Face 모델 및 OpenAI 스타일 API와 호환되며 GitHub에서 **4만 개 이상의 스타**를 받을 만큼 많은 개발자들의 사랑을 받고 있음

  

## **3. vLLM의 핵심 기능**

* **PagedAttention:** 운영체제의 가상 메모리 페이징 기법에서 영감을 받은 알고리즘. Attention의 Key-Value 캐시를 작은 블록(페이지) 단위로 분할하여 메모리 낭비를 **4% 이하로 감소**시킴

  * 고정 메모리를 예약하는 대신, 요청에 따라 **블록 단위로 동적으로 할당**
  * 각 요청 간 중복된 토큰 입력("What is the capital of")에 대해 **KV 블록을 재사용**하여 연산을 줄임

* **Continuous Batching:** 전통적인 정적 배치와 달리, 들어오는 요청을 **실시간으로 배치**에 추가하여 GPU 활용률을 극대화하고 지연을 줄임

  * 고객 지원 챗봇이 10개의 질문을 순차적으로 받는 경우, 정적 배치는 10개를 모을 때까지 기다리지만, **vLLM은 들어오는 즉시 처리**하여 응답 시간을 크게 줄임

* **양자화(Quantization) 지원:** 모델의 가중치와 활성값의 정밀도를 줄여 메모리 사용량과 계산량을 줄임

  * **PTQ** (사후 양자화): 훈련된 모델을 재학습 없이 빠르게 양자화
  * **QAT** (양자화 인지 훈련): 정확도를 높이기 위해 훈련 과정에 양자화를 통합
  * **혼합 정밀도(Mixed Precision)**: 가중치는 8비트, 활성값은 16비트 등 정밀도를 혼합 사용
  * 예시: LLaMA-13B를 FP16(26GB)에서 INT8(13GB)로 양자화하면 24GB GPU에서도 실행 가능하며, 정확도 손실은 최소화됨

* **최적화된 CUDA 커널:** FlashAttention, FlashInfer 등 GPU 연산을 위한 커스텀 CUDA 커널을 활용하여 **불필요한 연산을 제거하고 성능을 최대화**함

* **Tensor 병렬 처리 & 추론 가속 (Speculative Decoding)**

  * **Tensor Parallelism**: 모델을 여러 GPU에 분산하여 더 큰 모델 실행 가능

  * **Speculative Decoding**: 작은 모델로 빠르게 예측하고, 큰 모델이 이를 검증하여 추론 속도 향상

    

## **4. 참고 자료**

* vLLM 오픈소스: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

* vLLM 문서: [https://docs.vllm.ai/en/stable/](https://docs.vllm.ai/en/stable/)

* vLLM 논문: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

* vLLM 블로그: [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)

* AnyScale 블로그: [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](https://www.anyscale.com/blog/continuous-batching-llm-inference)

* Redhat 문서(Neural Magic 합병): [What is vLLM?](https://www.redhat.com/en/topics/ai/what-is-vllm)

  