---
title: "DeepSeek 모델 심층 분석"
date: 2025-03-11
tags: [답시크, DeepSeek, MoE, Mixture-of-Experts, DeepSeek-V3, DeepSeek-R1, chain-of-thought, 지도 미세조정, SFT, Multi-head Latent Attention, OpenAI, GPT-4, Claude, 클라우드, Cloud, 온프레미스, OnPremises]
typora-root-url: ../
---


중국의 인공지능 연구 기업인 DeepSeek 에서 세계 최첨단 AI 모델과 기술을 개발하며 오픈 소스 대형 언어 모델을 공개했다. 특히, DeepSeek V3는 대규모 언어 모델로 자연어 처리와 텍스트 기반 작업에 특화되어 있으며, DeepSeek R1은 논리적 추론과 문제 해결 능력에 중점을 둔 모델로 강화 학습을 통해 스스로 학습하고 발전하는 특징이 있다. 그렇다면, 모델 아키텍처 및 크기, 학습 방법론, 추론 성능과 인프라스트럭처, 하드웨어 및 비용 효율성 측면 등 다양한 각도에서 DeepSeek V3 와 R1에 대해 심층 분석해 보자. 



## 모델 아키텍처 및 크기

DeepSeek-V3는 *Mixture-of-Experts* (MoE) 구조를 채택한 초대형 언어 모델이다. 이 모델에 대한 총 파라미터는 6,710억 개지만, 질의마다 370억 개 정도의 파라미터만 활성화되어 효율성을 높이고 있다. 이러한 MoE 설계 덕분에 필요한 Expert만 가동되어 성능을 내면서도 계산 량을 줄일 수 있다. 

DeepSeek-R1은 내부적으로 *chain-of-thought* 추론 단계를 거치므로 응답이 느리지만 더 깊은 논리적 추론을 수행한다. 현재 AI 업계에서 학습과 추론의 성능을 극대화하기 위해, 멀티 노드 상에서 멀티 GPU 를 활용한 분산 학습이 표준으로 자리 잡고 있다. 이 과정에서 소프트웨어 엔지니어들은 **대규모 언어 모델을 어떻게 효율적으로 분산화할 것인가**라는 과제에 직면하게 된다.

## 학습 방법론: 데이터 및 RLHF 

DeepSeek V3과 R1은 각각 학습 방법이 다음과 같이 차이가 난다.  

* **DeepSeek V3**: 
  * 방대한 **14.8조 토큰** 규모의 멀티 도메인 데이터로 사전 학습되어 지식 범위가 매우 넓다. 
  * MoE 구조의 V3를 학습하는 데 **278만 GPU-시간**이 투입되었고, 학습 안정성을 높이기 위한 Multi-head Latent Attention 와  aux-loss 최적화 등과 같이 특별한 테크닉이 적용되었다.  
  * 사전 학습 후에는 **지도 미세조정(SFT)** 단계가 진행되어, 인간이 직접 작성한 양질의 답변 데이터를 통해 답변의 문법, 응집력, 사실성을 개선했다.
  * V3 자체는 주로 SFT로 응답 품질을 높였으며, 강화 학습을 통한 대규모 피드백(RLHF)은 R1 모델에서 본격 활용되었다.

* **DeepSeek R1**: 
  * V3 모델을 기반으로 **강화 학습 중심**의 새로운 학습 파이프라인을 거친 모델이다. 
  * R1의 학습은 **다단계**로 이루어졌는데, 먼저 **콜드스타트 미세조정**을 통해 소량의 고품질 데이터로 모델을 초기 튜닝했고, 이후에는 **인간 레이블 없이** 모델 스스로 강화학습(RL)을 수행하게 했음.  
  * 구체적으로, R1은 문제 해결을 위해 스스로 여러 해결안을 생성한 뒤, 정해진 규칙 기반 *보상 함수*로 답변의 정확도와 추론 과정을 평가받는 식으로 훈련되었습니다
  * 이렇게 *자기 시뮬레이션*을 통해 논리적 추론 능력을 키우는 한편, **거절 샘플링** 기법을 도입해 모델이 생성한 여러 응답 중 최상의 것만 추려 다시 학습시켰다.
  * V3의 SFT 데이터와 모델이 생성한 고품질 답변 데이터를 **블렌딩**하여 학습하고, **최종 RL 단계**에서 인간 선호에 맞도록 모델을 재조정함으로써 응답의 체계성과 품질을 높였다.
  * R1은 *대규모 강화학습 + 보강적 SFT*로 V3 대비 추론력을 극대화했으며, 이는 OpenAI GPT-4 수준의 문제 해결 성능을 훨씬 낮은 비용으로 달성하게 한 핵심 요인이다. 

## 추론 성능 (처리 속도 및 실시간 응답)

* DeepSeek-V3는 MoE 구조 덕분에 *추론 시 활성 파라미터가 적어* **속도가 빠른 편**이다. 
* DeepSeek는 V3를 개발하며 **추론 속도에서의 혁신적 도약**을 이루었다고 밝혔는데, V3 모델은 이전 세대보다 훨씬 빠른 응답을 보이며, 공개 LLM 중 최고 수준의 추론 처리량을 달성했음. 
* 실제로 6,700억 거대 모델이면서도 질의 당 370억 파라미터만 연산하므로 동일 연산 자원에서 GPT-4나 Claude보다 빠른 토큰 생성을 기대할 수 있다. 
* *Multi-head Latent Attention* 등 V3에 도입된 최적화 기법은 병렬 처리를 향상시켜 **대용량 입력에 대한 처리 효율**도 높인다.
* DeepSeek-V3는 응답 품질을 높이기 위해 한차례 출력만 생성하지만, **DeepSeek-R1**은 응답 전 내부 “Deep Thinking” 단계를 거치므로 **응답 지연**이 눈에 뛴다.
* Datacamp 실험에 따르면 R1은 답을 바로 내놓지 않고 수 분간 사고 과정을 수행한 뒤 답변을 출력하며, 일반적으로 V3보다 훨씬 **응답 시간이 길다**고 한다. 
* 실시간 대화에서는 V3가 적합하고, R1은 어려운 문제에서 대기 시간이 길더라도 심층 해설이 필요할 때 쓰는 식으로 역할이 구분된다. 

## 인프라스트럭처 (클라우드 vs 온프레미스)

* DeepSeek는 자사 클라우드 서비스로 V3와 R1 모델을 제공하고 있다. 
* 사용자는 DeepSeek 웹 앱이나 모바일 앱에서 V3 모델과 대화하거나, **DeepSeek API 플랫폼**을 통해 개발자용으로 모델을 활용할 수 있다.
* DeepSeek-V3는 현재 *클라우드 전용 서비스*로 제공되며, 모델 가중치가 공개되어 있지는 않는다. 
* 한편 **DeepSeek-R1**은 처음부터 연구 커뮤니티를 위해 오픈소스로 공개되었는데, R1 및 R1-Zero의 가중치와 추론 코드를 Hugging Face에 배포하여 누구나 로컬에서 실행해볼 수 있게 했다.
* 다만 R1 본체는 MoE 구조라 일반 사용자가 온프레미스에서 돌리기 어렵고, 대신 R1의 지식을 압축한 *distilled* 버전 (라마나 Qwen 기반의 32B 등)들을 함께 공개하여 비교적 적은 자원으로도 활용 가능하게 했다
* 기업용 온프레미스 지원 측면에서는 DeepSeek가 Microsoft와 협력 움직임을 보이고 있다. 예를 들어, **Azure AI Foundry**에 DeepSeek-R1이 등재되어 있다. 
* Azure 클라우드 이용 기업들이 R1을 쉽게 테스트해볼 수 있게 되었고, 이는 DeepSeek가 자체 클라우드뿐 아니라 타사 클라우드와도 연동을 모색하는 사례이다.
* 요약: DeepSeek는 **클라우드 우선** (자체 플랫폼 제공)이지만, **오픈소스 공개**를 통해 연구자 자율 실행도 지원하며, 향후 파트너십을 통해 다양한 인프라에 통합을 추진하고 있다. 

## 하드웨어 (GPU/TPU 및 추론 최적화)

* DeepSeek-V3 모델을 학습시키기 위해 막대한 GPU 리소스가 투입되었습니다. 보고에 따르면 V3의 사전학습에 **총 278만 GPU-시간**이 소요되었음. 예) 1,000개의 GPU를 동원하면 약 116일, 2,000개면 약 58일간 풀가동해야 하는 수준의 연산량이다. 
* 대규모 학습은 주로 NVIDIA의 A100 GPU 클러스터에서 이루어졌을 것으로 추정된다. DeepSeek는 중국 스타트업이지만, 모델 성능으로 미루어 미국산 A100/H100을 활용한 것으로 보이며, 구체적인 사용 GPU는 공개하지 않았다.
* DeepSeek 개발 팀은 *학습 안정성*을 위해 자체 MoE 프레임워크를 개선하고 *auxiliary loss-free balancing* 등의 기법으로 6,700 억급 모델의 **안정적인 수렴**을 달성했다고 논문에서 밝혔다.
* 추론 시에는 MoE 특성상 여러 전문가 파라미터를 불러와야 하므로 **분산 시스템**이 필수이다. DeepSeek-V3/R1은 370억 파라미터의 활성 부분을 16~32개 GPU에 나눠 띄워 동작하도록 설계되었으며, 노드 간 통신을 효율화하는 최적화도 적용되었습니다. 예) *FasterMoE*와 유사한 토큰-라우팅 최적화로 통신 부하를 줄였을 가능성이 있다.
* R1의 경우, OpenAI GPT-4 수준의 성능을 내면서도 훈련 비용이 훨씬 적게 들었다고 강조하는데, 이는 *모델 리파인먼트 중심의 RL* 덕분에 완전 처음부터 거대 모델을 학습시키는 비용을 절약했기 때문이다. R1 강화 학습에도 여전히 수백 대의 GPU가 사용되었겠지만, V3 사전 학습만큼 오래 걸리진 않았을 것이다. 
* **하드웨어 최적화 측면**에서 DeepSeek는 추론 시 *vLLM*과 유사한 메모리 최적 관리, 그리고 FP16 혼합정밀도 사용 등을 통해 비용을 절감했다. 
* R1-Distill 모델들은 LLaMA나 Qwen 기반이라, 일반적인 GPU에서 HF Transformers로 구동 가능하게 되어 있다. 
* 전반적으로 DeepSeek는 최신 NVIDIA GPU (A100/H100 계열)를 적극 활용하여 학습/추론을 하고 있으며, 하드웨어 제약을 극복하기 위해 **MoE 아키텍처+분산 최적화**라는 전략을 취했다.

## 비용 효율성 (학습 & 추론 비용, API 요금)

* DeepSeek는 “저비용 고성능”을 강조하며 OpenAI 대비 **비용 효율적 전략**을 취했음.  
* 학습 측면에서, V3와 R1 개발 총 비용은 공개되지 않았지만, V3의 278만 GPU-시간과 자체 MoE 기술 개발 등을 감안하면 수 십억 원대에 이를 것으로 추정. 그럼에도 불구하고 OpenAI GPT-4의 추정 비용($1억+)과 비교하면 현저히 낮은 편. **기존 공개 모델(예: LLaMA, Qwen)의 재사용 및 강화학습 활용**으로 완전 신규 학습 비용을 줄였기 때문
* 실제로 DeepSeek-R1은 LLaMA/Qwen 기반 32B 압축 모델을 산출하며, “OpenAI o1-mini” 수준 모델을 훨씬 적은 비용에 얻었다고 보고하고 있음. 
* **추론 비용**에서는 DeepSeek가 시장을 놀라게 할 만큼 공격적인 가격 정책을 폈음. **DeepSeek-V3 (chat)** 모델은 2023년 프로모션 기간 이후 기준으로 *100만 입력 토큰 당 $0.27, 100만 출력 토큰 당 $1.1* 수준의 요금을 책정했음. 토큰 1개로 환산하면 입력 $0.00027, 출력 $0.0011로, **GPT-4의 1/20 이하**에 불과함. 
* 더 복잡한 **DeepSeek-R1** 모델의 경우에도 *100만 토큰 입력 $0.55, 출력 $2.19* 정도로, GPT-4 대비 *10~50배 저렴한* 파격적 가격임. 예) GPT-4의 출력 비용은 100만 토큰당 약 $60 (1토큰 $0.06)인데, DeepSeek-R1은 동일 기준 $2.19에 불過ぎ하여 거의 **30배 이상의 비용 절감**을 이룸. 
* 이렇게 낮은 API 요금은 DeepSeek가 초기 시장 점유를 위해 부담을 감수하고 책정한 것으로 보이며, 실제 성능이 GPT-4급인 R1을 이 가격에 제공함으로써, 일부 개발자들은 “게임체인저”라는 평가를 했고 시장 경쟁을 촉발했음.
* DeepSeek 측은 MoE 구조로 **추론 계산량을 줄인 것**과, 자체 데이터센터/클라우드 최적화를 통해 단가를 낮출 수 있었다고 언급함. 향후 가격 변동 가능성은 있지만, 현 시점에서 DeepSeek은 **동급 성능 대비 최저 비용** LLM 서비스를 제공하고 있음. (또한 DeepSeek 웹/앱은 무료로 V3 모델을 체험할 수 있게 해 두어, 개인 사용자는 비용 없이도 혜택을 볼 수 있음)



