---
title: "초거대 언어 모델에서의 분산 학습"
date: 2023-12-08
tags: [NVIDIA, GPU, CUDA, NCCL]
typora-root-url: ../
---



트랜스포머 기반의 대규모 언어 모델을 학습하고 이를 프로덕션 환경에 배포하는 과정에는 여러 가지 도전 과제가 존재한다. 흔히 초거대 언어 모델에서는 대규모 텍스트 데이터를 기반으로 단어와 문장의 패턴을 학습하여 언어를 이해하고 생성할 수 있도록 모델의 파라미터를 조정하는 과정인 **훈련(학습)**과 훈련된 모델이 입력에 따라 가장 그럴듯한 다음 단어나 문장을 예측하여 생성하는 과정인 **추론**으로 나눈다. 

학습 단계에서는 모델이 사용 가능한 GPU 메모리보다 많은 메모리를 요구하거나 학습 속도가 느릴 수 있다. 반면, 배포 단계에서는 프로덕션 환경에서 요구되는 처리량을 모델이 감당하지 못하는 문제가 발생할 수 있다.

이러한 문제들을 극복하고 사용 사례에 최적화된 설정을 찾는 데, 학습 시 단일 GPU을 사용할 떄와 다중 GPU 사용할 때, 또한 이러한 GPU를 하나의 노드에서 사용할 때와 여러 개의 노드를 사용할 때 각각 솔루션이 다르다.  추론 시 CPU와 GPU 등 다양한 하드웨어 구성에 따라 마찬가지이다. 그렇다면, 간단하게 한번 정리해보자! 



## 1. 훈련

* 트랜스포머 기반의 초거대 언어 모델을 효율적으로 훈련하려면, GPU나 TPU와 같은 가속기가 필요함. 
* 가장 일반적인 경우는 단일 GPU를 사용하는 경우이고, 단일 GPU에서 훈련 효율을 개선하기 위해 적용할 수 있는 방법들은 여러 개의 GPU와 같은 다른 구성에서도 확장할 수 있다. 
  - [단일 GPU에서 효율적인 학습을 위한 방법과 도구](https://github.com/synabreu/nvidia-note/blob/main/Huggingface-PS/perf_train_gpu_one.md): GPU 메모리 활용도를 최적화하거나 학습 속도를 높이는 일반적인 접근 방식을 설명함.
  - [다중 GPU 를 이용한 훈련 방법](https://huggingface.co/docs/transformers/v4.48.2/perf_train_gpu_many): 데이터, 텐서, 파이프라인 병렬 처리와 같이 다중 GPU 환경에 적용할 수 있는 추가 최적화 방법을 설명함.
  - [CPU 훈련 영역](https://huggingface.co/docs/transformers/v4.48.2/perf_train_cpu): CPU에서의 혼합 정밀도 학습
  - [여러 CPU에서의 효율적인 훈련](https://huggingface.co/docs/transformers/v4.48.2/perf_train_cpu_many): 분산 CPU 학습
  - [TensorFlow를 활용한 TPU 훈련](https://huggingface.co/docs/transformers/v4.48.2/perf_train_tpu_tf): TPU 사용이 처음이라면, TPU 학습 및 XLA 사용에 대한 방법 소개
  - [훈련용 맞춤 하드웨어](https://huggingface.co/docs/transformers/v4.48.2/perf_hardware): 자신만의 딥러닝 장비를 구축할 때
  - [Trainer API를 활용한 하이퍼파라미터 탐색](https://huggingface.co/docs/transformers/v4.48.2/hpo_train)



## 2. 추론

* 대규모 언어 모델을 프로덕션 환경에서 효율적으로 추론하는 것은 훈련 만큼이나 어려울 수 있다. 아래의 부분에서는 CPU 및 단일/다중 GPU 환경에서 추론을 수행하는 방법을 정리했다.
  - [단일 CPU에서의 추론](https://huggingface.co/docs/transformers/v4.48.2/en/perf_infer_cpu)
  - [단일 GPU에서의 추론](https://huggingface.co/docs/transformers/v4.48.2/en/perf_infer_gpu_one)
  - [다중 GPU 추론](https://huggingface.co/docs/transformers/v4.48.2/en/perf_infer_gpu_multi)
  - [추론 디버깅](https://huggingface.co/docs/transformers/v4.48.2/en/debugging)
  - [torch.compile()를 이용한 추론 최적화](https://huggingface.co/docs/transformers/v4.48.2/en/perf_torch_compile)
  - [대형 모델 인스턴스화](https://huggingface.co/docs/transformers/v4.48.2/en/big_models)
  - [TensorFlow 모델의 XLA 통합](https://huggingface.co/docs/transformers/v4.48.2/en/tf_xla)
