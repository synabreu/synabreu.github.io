---
title: "ZeRO 방식은 DDP 확장 개념인가"
date: 2024-01-06
tags: [DeepSpeed, 딥스피드, Microsoft, 마이크로소프트, PyTorch, ZeRO optimizer, Mixed Precision, Model Parallelism, Pipeline Parallelism, DeepSpeed-Inference, DDP, Distributed Data Parallel]
typora-root-url: ../
---



ZeRO는 DDP의 확장 개념으로 볼 수 있다. 그러나 단순한 확장이라기보다는 DDP의 구조적 한계를 근본적으로 개선한 방식이다.  DDP와 Zero 와의 비교를 하자면 다음과 같다.



## 1. DDP vs ZeRO 비교

| 항목             | DDP (DistributedDataParallel)      | ZeRO (Zero Redundancy Optimizer)                         |
| ---------------- | ---------------------------------- | -------------------------------------------------------- |
| 모델 복제 방식   | **모든 GPU에 모델 전체 복제**      | **모델 상태를 GPU 간 분산 저장**                         |
| 통신 방식        | AllReduce (그래디언트 전체 동기화) | ReduceScatter + AllGather (상태별 분산 통신)             |
| 확장성 한계      | 모델이 커질수록 **메모리 병목**    | 초대규모 모델도 수천억 파라미터까지 학습 가능            |
| 적용 대상        | 주로 그래디언트 동기화             | 그래디언트, 옵티마이저 상태, 파라미터까지 모두 분산 저장 |
| 기반 기술        | NCCL, Gloo 등                      | 내부적으로 NCCL 기반, but 엔진 레벨에서 제어             |
| DeepSpeed 종속성 | ❌ PyTorch native                   | ✅ DeepSpeed 내부 엔진 필수                               |

* DDP
  * 모든 GPU에 **동일한 모델 전체를 복제**
  * forward/backward 마다 모든 그래디언트를 AllReduce 함
  * 모델이 1B~10B 이상일 경우 **GPU 메모리가 금방 포화됨**
* ZeRO
  * **모델 상태를 GPU 간에 분산 저장** (옵티마이저, 파라미터, 그래디언트)
  * 단계별로 (Stage 1~3) 점진적으로 메모리 최적화
  * DDP의 통신 및 메모리 병목을 해소하기 위한 확장 전략



즉, **ZeRO는 DDP 위에서 작동하지만, 단순한 wrapping이 아니라 구조적 개선을 추가한 병렬 학습 프레임워크**



## 2. DDP 와 Zero 플로우

```mathematica
          ┌─────────────┐
          │  DDP 기반    │
          └─────┬───────┘
                ▼
         ┌──────────────┐
         │   ZeRO Stage 1│──▶ Optimizer State 분산
         └──────────────┘
                ▼
         ┌──────────────┐
         │   ZeRO Stage 2│──▶ + Gradient 분산
         └──────────────┘
                ▼
         ┌──────────────┐
         │   ZeRO Stage 3│──▶ + Parameter 분산 (Full Shard)
         └──────────────┘

```



## 3. 결론

| 질문                    | 답변                                                         |
| ----------------------- | ------------------------------------------------------------ |
| ZeRO는 DDP의 확장인가?  | ✅ 네, DDP 위에 구축된 확장 최적화 기법입니다.                |
| 완전히 독립된 방식인가? | ❌ 아닙니다. DDP를 내부적으로 사용하고, 구조적으로 개선한 방식입니다. |
| 대규모 모델에 적합한가? | ✅ ZeRO-3는 수천억 파라미터 모델도 단일 노드/다중 노드에서 훈련 가능하게 합니다. |