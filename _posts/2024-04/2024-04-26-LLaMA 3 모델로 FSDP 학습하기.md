---
title: "LLaMA 3 ëª¨ë¸ë¡œ FSDP í•™ìŠµí•˜ê¸°"
date: 2024-04-26
tags: [ë©”íƒ€, llama, FSDP, PyTorch, ZeRO optimizer, Mixed Precision, Model Parallelism, Pipeline Parallelism, DeepSpeed-Inference, DDP, Distributed Data Parallel]
typora-root-url: ../
toc: true
---



ë©”íƒ€ ë¼ë§ˆ3ê°€ ì¶œì‹œë˜ê°€ ë˜ì–´ì„œ  meta-llama/Llama-3-8B ëª¨ë¸ì„ ê°€ì§€ê³  FSDP ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì „ì²´ ê³¼ì •ì„ **ë‹¨ê³„ë³„ë¡œ** ì •ë¦¬í•´ë³´ì•˜ë‹¤. ì´ íŠœí† ë¦¬ì–¼ì€ PyTorch ê¸°ë°˜ì´ë©°, ì£¼ë¡œ Hugging Face Transformersì™€ ğŸ¤— Accelerate ì—†ì´ **ì§ì ‘ FSDPë¥¼ êµ¬ì„±**í•˜ëŠ” ë°©ì‹ì´ë‹¤.



## 1. ì „ì œ ì¡°ê±´ 

* PyTorch >= 2.0
* GPU 2ê°œ ì´ìƒ (ì˜ˆ: A100, H100)
* NCCL backend
* ëª¨ë¸: `meta-llama/Llama-3-8B` ë˜ëŠ” `Llama-3-3B`
* OS: Ubuntu, Python 3.10+



## 2. ì „ì²´ êµ¬ì„± ìš”ì•½

* í™˜ê²½ ì„¤ì • (Public Cloud / On-Prem ë“±) ë° ì‹¤í–‰
* ëª¨ë¸ ë¡œë”© ë° í† í¬ë‚˜ì´ì € ì„¤ì •
* FSDP ì´ˆê¸°í™” ë° êµ¬í˜„
* ë°ì´í„°ì…‹ ë¡œë”© ë° ìƒ˜í”ŒëŸ¬ êµ¬ì„±
* í•™ìŠµ ë£¨í”„ êµ¬í˜„ (forward, loss, backward)
* FSDPì—ì„œì˜ ì €ì¥ ë° ë¡œë”©



## 3. í™˜ê²½ ì„¤ì • ë° ì‹¤í–‰

* 2ê°œ GPU ì„¤ì •

```bash
# ì˜ˆì‹œ: torchrun ë°©ì‹ (NCCL backend)
torchrun \
  --nproc_per_node=2 \
  --nnodes=1 \
  --rdzv_id=101 \
  --rdzv_backend=c10d \
  --rdzv_endpoint=localhost:29500 \
  llama3_fsdp_train.py
```



## 3. ë¼ë§ˆ3 ëª¨ë¸ ë° í† í°ë‚˜ì´ì € ì‹¤í–‰

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "meta-llama/Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
```



## 4. FSDP ì´ˆê¸°í™” ë° êµ¬í˜„

```python
import torch
import torch.distributed as dist
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from transformers.models.llama.modeling_llama import LlamaDecoderLayer

def fsdp_wrap_model(model, rank):
    auto_wrap_policy = transformer_auto_wrap_policy({LlamaDecoderLayer})
    model = model.to(rank)
    fsdp_model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        device_id=rank,
        sharding_strategy=torch.distributed.fsdp.ShardingStrategy.FULL_SHARD,
        mixed_precision=torch.distributed.fsdp.MixedPrecision(param_dtype=torch.bfloat16)
    )
    return fsdp_model
```



## 5. ë°ì´í„°ì…‹ ë¡œë”©

```python
from datasets import load_dataset

# Hugging Faceì— ìˆëŠ” allganize ì˜ Finanacial dataset
dataset = load_dataset("allganize/financial-mmlu-ko", split="train")
def tokenize_function(example):
    return tokenizer(example["input"], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

```



## 6. Dataloader ë° Sampler

```python
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

sampler = DistributedSampler(tokenized_dataset)
dataloader = DataLoader(tokenized_dataset, batch_size=2, sampler=sampler)
```



## 7. í•™ìŠµ ë£¨í”„ (FSDP + Optimizer)

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

model.train()
for epoch in range(3):
    for batch in dataloader:
        input_ids = batch["input_ids"].to(rank)
        labels = batch["input_ids"].to(rank)  # causal LM

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if dist.get_rank() == 0:
            print(f"Loss: {loss.item()}")

```



## 8. FSDP ëª¨ë¸ ì €ì¥

```python
# ì €ì¥
torch.distributed.barrier()
FSDP.save_model(model, "llama3-fsdp-checkpoint/")
```



## 9. FSDP ëª¨ë¸ ë¡œë“œ

```python
# ë¡œë”©
fsdp_model = FSDP(model, device=rank)
FSDP.load_model(fsdp_model, "llama3-fsdp-checkpoint/")
```



## 10. ì¶”ê°€ íŒ

* `activation_checkpointing`ì„ FSDPì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ VRAM ì‚¬ìš©ëŸ‰ì„ ë” ì¤„ì¼ ìˆ˜ ìˆìŒ
* FSDPëŠ” 3D parallelism (Tensor + Pipeline + Data)ê³¼ë„ ë³‘í–‰ ê°€ëŠ¥
* `megatron-lm`, `fairscale`, `DeepSpeed` ë“±ê³¼ì˜ ë¹„êµëŠ” ì˜µì…˜ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ ì¡´ì¬



