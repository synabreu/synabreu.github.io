---
title: "LLM 추론 시 메모리 크기 측정"
date: 2024-04-20
tags: [메타, 라마, Meta, LLama, 텐서, 훈련, Training, 추론, Inference, GPU, 모델파라미터, H100, A100, PyTorch, Transformer, Ampere, Hopper]
typora-root-url: ../
---

이틀 전 4월 18일에 메타에서 대규모 언어 모델 Llama 3를 업그레이드 발표했다. Meta의 Llama 3 시리즈는 다양한 규모의 대규모 언어 모델(LLM)로 구성되어 있으며, 각 모델은 특정 용도와 성능 요구에 맞게 설계되었다. 따라서, 간단하게 Llama 3 발표한 내용을 토대로 종류와 주요 특징을 다음과 같이 정리한다. 

| 모델 명         | 파라미터 수 | 특징 및 용도                                                 |
| --------------- | ----------- | ------------------------------------------------------------ |
| Llama 3 8B      | 8억 개      | \- 경량화된 모델로, 로컬 환경이나 모바일 디바이스에서의 실행에 적합함 <br/> \- 텍스트 생성 및 간단한 응용 프로그램에 활용함. |
| Llama 3 70B     | 700억 개    | \- 중간 규모의 모델로, 다양한 언어 처리 작업에 균형 잡힌 성능을 제공. <br/> \- 상업적 응용 프로그램 및 연구에 활용 |
| Llama 3.1 405B* | 4,050억 개  | \- 추후 공개될 모델로  메타에서 가장 큰 규모로, 복잡한 언어 이해 및 생성 작업에 탁월한 성능을 보임 <br/> \- 고성능 컴퓨팅 자원이 필요하며, 대규모 데이터 처리 및 고급 연구에 적합함. |

그러나, 우리의 오늘 주제는 라마의 주요 특징을 알아보는 것이 아니다. **meta-llama/Meta-Llama-3-8B-Instruct에서 Training 할 때, Tensor Type으로 BF16 이면 메모리 크기는 얼마이며, 몇 장의 GPU를 사용해야 하는가**에 대해 알아 볼 것이다. 사실 국내 대기업 인프라 팀들을 만나면 가장 많이 물어 보는 질문들 중 하나이다. 



## 1. Meta-Llama-3-8B-Instruct에서 훈련 시 메모리 크기 및 GPU 장수는? 

* **메모리 사용량 개략적 계산 = (모델 파라미터 * 2) +  파라미터의 2 - 3배** 
  * BF16(bfloat16)은 FP16과 마찬가지로 16비트 정밀도를 사용하므로, 기본적으로 모델 파라미터*자체가 차지하는 메모리는 FP16 대비 큰 차이가 없음.
  * AdamW 와 같은 옵티마이저 상태나 그래디언트, 중간 활성화 (activations) 등으로 인해 실제 요구되는 메모리는 2~3배 가량 될 수 있음.
  * 계산식: (16 * 2)  + (16GB * 2 or 16GB * 3) = 32GB + (32 or 48 )GB = 64 or 80 GB
* BF16 타입으로 풀 파인튜닝(Full fine-tuning)시 
  * **단일 GPU**으로는 최소 40GB 이상, 안정적인 배치 크기 확보를 위해서는 **A100/H100 80GB급이 1장**이 권장
  * **80GB H100 1장**으로 분산 학습을 활용해도 충분히 학습이 가능함. 



## 2. Meta-Llama-3-8B-Instruct에서 추론 시 메모리 크기 및 GPU 장수는? 

* **메모리 사용량** 
  * 모델 파라미터
    * 매개변수(Parameter) 수: 약 80억(8B) 
    * BF16(2바이트)로 저장 시: 8 Billion × 2 Bytes≈16 GB 
  * 추가 메모리(오버헤드)
    * 추론 시  활성화(Activation)와 캐시(cache) 등이 필요하지만, *학습*보다는 훨씬 적은 양의 메모리를 사용함. 
    * 프롬프트 길이(문맥 길이), 배치 크기에 따라 달라지나, 일반적으로 **수GB** 정도의 여유가 필요함. 보통 안전하게 **20GB 전후**의 GPU 메모리가 권장. 
    * AdamW 와 같은 옵티마이저 상태나 그래디언트, 중간 활성화 (activations) 등으로 인해 실제 요구되는 메모리는 2~3배 가량 될 수 있음.
  * **단일 GPU에서** BF16 추론을 원활하게 수행하려면, **대략 20GB 이상의 VRAM**을 갖춘 GPU가 필요함.  
    * 예: 24GB급(예: RTX  3090, RTX 4090, A6000) 이상이면 단일 GPU로 충분히 가능
  * 만약 **16GB급 GPU**에서 시도한다면,
    * 컨텍스트 길이나 동시 추론(batch) 크기를 작게 조정하거나,
    * 8비트/4비트 양자화(quantization) 같은 기법을 추가로 활용해야 할 수 있음. 
* **권장 GPU 수**
  * **1장**으로도 충분히 추론이 가능하지만,
  * 여러 배치 요청이나 긴 시퀀스, 다양한 파이프라인 처리를 동시에 해야 한다면, **여러 장의 GPU**를 사용하는 멀티 GPU 환경(예: 데이터 병렬)을 구성할 수 있음. 
* **결론**
  * 보통은 8B급 모델에서는 성능(처리 속도)을 높이기 위해서 여러 장을 쓰기보다는 **단일 고용량 GPU(예: 24GB+)** 한 장을 사용하는 경우가 많음. 

 

## 3. 필요 구성 요소 

* **BF16 지원 GPU** 

  * 모델 파라미터
    * Ampere 아키텍처(A100, RTX 3090, RTX 4090 등) 또는 Hopper(H100), Ada Lovelace(RTX 6000 Ada) 계열 이상 권장
    * 구형 GPU(V100, T4 등)는 BF16 지원이 제한적이거나 성능이 떨어질 수 있음 

* **추론 프레임워크 및 라이브러리**

  * PyTorch(최신 버전), Transformers(Hugging Face)
  * 모델 로딩 시 torch.bfloat16 또는 auto_dtype="bf16" 형태로 설정하여 BF16 추론 활성화

* **소프트웨어/환경 설정**

  * CUDA와 PyTorch 버전이 해당 GPU와 호환되어야 함
  * BF16 연산을 지원하도록 프레임워크 및 드라이버 세팅 필요
  * 가능하다면 Docker 또는 Conda 환경을 통해 의존성 버전 고정 및 재현성 확보

* **모델 파일**

  * meta-llama/Meta-Llama-3-8B-Instruct 모델 가중치(Weights)
  * 모델 아키텍처를 불러오고 BF16 모드로 변환할 수 있는 스크립트(Transformers  예시 등)

  

## 4. 정리 

* **메모리**: BF16 상태에서 **모델 자체**가 약 16GB, 추가 오버헤드 감안 시 **20GB 이상** 권장
* **GPU 수**: 단일 24GB급 GPU 1장으로도 추론이 가능함
* **필요 요소**: 
  * BF16 연산이 가능한 **Ampere급 이상 GPU**
  * **PyTorch, Transformers** 등 BF16 지원 라이브러리
  * CUDA/드라이버 호환 환경
  * meta-llama/Meta-Llama-3-8B-Instruct 모델 가중치 및 로딩 스크립트
