---
title: "NVIDIA NIM(3): NVIDIA API 카탈로그 탐험하기"
date: 2025-01-26
tags: [NVIDIA, NIM,GPU, CUDA, microservice, NVIDIA API, meta, llama, 쿠버네티스, 마이크로서비스, 추론, 메타, 라마]
typora-root-url: ../
toc: true
categories: [NVIDIA, NIM]
---

NVIDIA NIM은 NVIDIA API 카탈로그에서 마우스 몇 번만 클릭하면 GPU 기반 LLM을 테스트할 수 있는 NVIDIA의 클라우드 API 서비스이다. 현재  LLaMA, Mixtral, Gemma, DeepSeek 등 다양한 최신 모델 사용 가능하며, 개발자 친화적인 인터페이스와 무료 크레딧 제공을 제공한다. 그러면, NIM 온라인 웹사이트에서 여러분이 직접 따라해 볼 수 있도록 단계별로 설명하겠다. 



## **실습: NVIDIA API 카탈로그 탐험**

- 먼저 NIM을 사용하기 위해서는 [그림 1]과 같이 NVIDIA NIM을 호스트하고 있는 [이 웹사이트 주소로](https://build.nvidia.com/explore/discover) 먼저 접속하기 바란다.

![그림 1 - NVIDIA API 카탈로그 및 빌드 웹사이트](../images/2025-01/NIM-2-01.png)

<div align="center">[그림 1 - NVIDIA API 카탈로그 및 빌드 웹사이트]</div>



* NVIDIA NIM 을 사용하기 위해서는 사용자는 [그림 2]와 같이 먼저 Login 버튼을 눌러야 한다. 그러면, 다이얼로그 화면에 무료로 API 를 사용할 수 있는 비즈니스 전자 메일과 개인 메일에서 어떤 기능을 사용할 수 있는지 간단한 비교와 함께 여러분의 로그인에 사용할 전자메일 주소를 을 입력하는 란이 나타난다. 이때 텍스트 박스에 여러분의 전자메일 주소를 입력하고 [Next] 버튼을 누른다. 

![그림 2 - NVIDIA NIM 전자 메일 입력](../images/2025-01/NIM-2-02.png)

<div align="center">[그림 2 - NVIDIA NIM 전자 메일 입력]</div>



* [그림 3]은 여러분의 사용자 로그인 화면이다. [그림 2]에서 입력한 전자메일 주소가 자동으로 나오고, 여러분들은 Password만 입력해주면 된다. 그런 후 [Log In] 버튼을 눌러주기 바란다. 혹시 Password 가 기억나지 않는다면, [Forgot Password?] 링크를 클릭을 눌러 해당 사항에 대해 입력하면 된다. 

![그림 3 - NVIDIA Account 사용자 로그인 ](../images/2025-01/NIM-2-03.png)

<div align="center">[그림 3 - NVIDIA AccouNT 사용자 로그인]</div>



* 사용자 로그인이 성공적으로 끝마쳤다면, [그림 4]에서 보는 것처럼 메인 화면을 보게 될 것이다. 계속해서 NVIDIA가 모델을 업데이트하고 있기 때문에 여러분이 보는 시점에 따라 조금씩 메뉴는 변한다. 여기에서 우리는 오른쪽 위 모서리에 있는 사용자 프로필을 눌러 1,000 Credit 이 남아 있는지 먼저 확인하기 바란다.  

![그림 4 - 크레딧 확인](../images/2025-01/NIM-2-04.png)

<div align="center">[그림 4 - 크레딕 확인]</div>



* 자, 이제 부터 본격적으로 NIM으로 사용할 모델을 검색해 보자! [그림 5] 처럼, 검색 창에서 <mark style="background-color: lightgray;">llama'</mark>로 입력하면, 리스트 박스에서 현재 NIM 에서 제공하는 llama 모델이 파라미터 종류별 올라와져 있다. 여기에서 우리는 <mark style="background-color: lightgray;">llama-3.2-1b-Instruct</mark> 모델을 하나 선택하기 바란다.  

![그림 5 - NIM에서 llama 3.2 모델 검색 및 선택](../images/2025-01/NIM-2-05.png)

<div align="center">[그림 5 - NIM에서 llama 3.2 모델 검색 및 선택]</div>



* 그러면, [그림 6]과 같이 meta/llama-3-2-1b-instruct 모델의 상세 페이지가 나온다. 여기 화면에서는 모델의 간단한 설명과 더불어, 모델카드를 확인할 수 있고, 오른쪽 화면에서 간단하게 코드를 동작시키면, 해당되는 결과를 왼쪽 Preview 화면에서 보거나, JSON 형태로 보여준다. 여기에서 우리는 [Get API Key] 링크를 눌러 NIM 서비스에서 이 모델을 사용하기 위한 **인증 토큰(API Key)**을 발급받는 절차를 진행한다. 

![그림 6 - API Key 받기](../images/2025-01/NIM-2-06.png)

<div align="center">[그림 6 - API Key 받기]</div>



* 그렇다면, [그림 7]과 같이, 우리가 사용할 Token Key를 생성하기 위해 [Generate Key] 버튼을 누르면 된다. 

![그림 7 - API 키 생성하기](../images/2025-01/NIM-2-07.png)

<div align="center">[그림 7 - API 키 생성하기]</div>



* [그림 8]은 생성된 API를 보여주는 화면이다. 여러분이 생성한 Key는 잃어버리지 않기 위해 [Copy Key] 버튼을 눌러 다른 파일에 잘 저장해서 보관해둔다. 그러면 이 Key를 앞으로 여러분의 소스 코드에서 NIM API를 호출하기 전에 API 인증을 한다.

![그림 8 - 생성된 키 복사하기](../images/2025-01/NIM-2-08.png)

<div align="center">[그림 8 - 생성된 키 복사하기]</div>



* 이제 복사한 키를 오른쪽 소스 코드 화면에서 Python 을 선택하고, 소스 코드의 <mark style="background-color: lightgray;">api_key =복사한 키 붙여넣기</mark> 부분 안에 복사한 키를 붙여넣기 한다.  그리고 오른쪽 Preview 화면의 프롬프트 창에서 <mark style="background-color: lightgray;">What is GPU? </mark>라고 입력한다음, [Send] 버튼을 누른다. 

![그림 9 - 복사한 키 붙여넣기 및 프롬프트 창 입력](../images/2025-01/NIM-2-09.png)

<div align="center">[그림 9 - 복사한 키 붙여넣기 및 프롬프트 창 입력]</div>



* 그러면, [그림 10]과 같이 프롬프트 창에서 입력한 결과 대로 답변을 하는 모습을 볼 수 있다. 

![그림 10 - 프롬프트 입력 결과 화면](../images/2025-01/NIM-2-10.png)

<div align="center">[그림 10 - 프롬프트 입력 결과 화면]</div>



* 참고로 [그림 11]은 프롬프트 창 밑에 좀더 세밀하게 파라미터를 조정할 수 있도록 View Parameters 화면이다. 관련된 파라미터 옵션을 조정할 수 있다. 

![그림 11 - 모델 파라미터 조정](../images/2025-01/NIM-2-11.png)

<div align="center">[그림 11 - 그림 11 - 모델 파라미터 조정]</div>



이러한 옵션들은 **모델이 추론(inference) 시 텍스트를 생성하는 방식**을 조절하는 역할하며, 주로 디코딩 설정 또는 생성 제어 파라미터라고 부른다. 이러한 파라미터는 모델의 학습된 가중치나 파라미터와는 무관하다. 주요 생성 제어 파라미터를 요약하면 다음과 같다. 



| 항목              | 기본값 | 설명                                                         |
| ----------------- | -------- | ---------------------------------------------------------- |
| temperature       | 0.25     | 텍스트 생성을 위해 확률 분포를 얼마나 무작위로 샘플링할지 결정하는 온도 값 (**높을수록 다양하고 예측이 불가능, temperature 와 top_p 값을 동시에 조정하는 것은 권장하지 않음**.) |
| top_p             | 0.75     | nucleus sampling. 누적 확률이 p 이하인 상위 토큰들만 고려해 샘플링. 텍스트 생성을 위해 사용되는 **Top-p 샘플링 확률 질량(mass)** 값이며, `top_p` 값은 **샘플링 시 고려할 누적 확률의 범위**를 결정. 예) `top_p = 0.75`로 설정하면, **가장 가능성이 낮은 토큰들 중 누적 확률이 0.75에 도달할 때까지만** 선택 후보로 사용 |
| frequency_penalty | 0        | 자주 등장한 단어의 확률을 낮춰 반복 억제. 지금까지 생성된 텍스트에서 **등장 빈도(frequency)**를 기준으로, **새로운 토큰에 얼마나 패널티를 줄지**를 나타냄. **같은 문장을 그대로 반복할 가능성을 줄이기 위해** 모델의 출력 확률을 낮춤. `0`일 경우, **같은 단어를 반복하는 현상**이 생길 수 있음. |
| presence_penalty  | 0        | 이전에 등장한 단어 자체에 패널티를 주어 새로운 단어 사용 유도. 양의 값을 사용하면, **지금까지 생성된 텍스트에 이미 등장한 토큰**에 대해 **패널티를 부여**함. 모델은 **새로운 주제에 대해 이야기할 가능성**이 높아짐. `0`일 경우, **문장을 반복하는 현상**이 생길 수 있음. |
| max_tokens        | 1,025    | 한 번의 호출에서 생성할 최대 토큰 수 (출력 길이 제한). 모델이 한 번의 요청에서 생성할 수 있는 최대 토큰 수가 1,025개. 이 값은 **모델이 인식하는 것이 아니라**, 지정된 토큰 수에 도달하면 **단순히 생성을 중단**하는 방식으로 동작함. |
| stop              |         | 특정 문자열을 만나면 생성을 멈추는 조건. API가 **더 이상 토큰 생성을 중단할 시점**을 지정하는 문자열(또는 문자열 목록). 지정된 **`stop` 시퀀스가 등장하면 즉시 생성을 멈추며**, 반환되는 텍스트에는 **해당 `stop` 문자열이 포함되지 않음** |





또한, Stream 여부는 LLM의 **`stream`** 파라미터는 위에서 설명한 생성 제어 파라미터(generation parameter) 중 하나라기보다는, "응답 전달 방식"을 제어하는 출력 방식 설정값(output behavior parameter)이다.  LLM이 생성한 텍스트를 **한 번에 모두 반환할지**, 또는 **조금씩 나누어(스트리밍)** 반환할지를 제어하는 용도이다. 

기본값은 보통 `false` 이며, 한 번에 전체 출력을 한다. 만일 `stream=True`로 설정하면, 토큰이 생성될 때마다 **실시간으로** 출력한다. 예를 들어, ChatGPT, OpenAI API, LangChain 등에서 토큰 단위로 빠르게 출력되는 효과가 있다.



## **마무리**

- 우리는 지금까지 [NIM 웹사이트](https://build.nvidia.com/explore/discover)에 접속해서 사용자 로그인 한 다음,  llama-3.2-1b-Instruct` 모델을 선택 후, 토큰 인증을 위해 API 키를 생성한 다음, 왼쪽 프롬프트 창에서 간단히 입력 테스트를 해보고, 디코딩 설정하는 파라미터에 대해 알아보았다. 