2022년 11월, 오픈AI의 챗GPT가 공개되고 난 이후로 부터, 오픈AI, 구글, 마이크로소프트, 페이스북과 같은 대기업부터 미스트랄, xAI 과 같은 스타트업까지 기업 자체 연구소에서 초거대 언어 모델을 파운데이션 모델로 만들어 공개하는 것이 지난 2년 동안 전세계적 유행이 되었다. 따라서, 2023년과 2024년이 AI 훈련의 해였다면, 2025년 부터는 AI 추론의 해가 될 것이다. 텍스트나 이미지, 음성, 동영상과 같은 모든 데이터 훈련을 기반으로 새로운 가치를 제공하는 AI 추론은 성숙해가는 인공지능 시장과 일반 사람들의 실용적인 일상에서 AI 애플리케이션의 성공의 열쇠가 될 것이기 때문이다.

## 추론 성능 향상시키는 주요 요인

이러한 추론 성능을 향상시키는 주요 원인은, 지난 해 부터 엔비디아는 이전 호퍼(Hopper) 아키텍처보다 4배 더 높은 추론 성능을 자랑하는 블랙웰(Blackwell) GPU 시스템을 발표했다. 또한, 점점 더 많은 AI 에이전트가 AI 프로세스를 시작하기 위해 수백만 개의 토큰을 생성하기 시작함에 따라 추론 비용을 줄이는 능력에 있다. 따라서, 이러한 비용 절감은, 이른바 **"수익성 AI(Profitable AI)"**로 가는 길을 열어줄 것이다. 엔비디아의 가속 컴퓨팅 제품 디렉터인 데이브 살바토르(Dave Salvator)는 이러한 수익성 AI에 대한 근거를 다음과 같이 뒷받침했다.

> AI 추론은 처리량과 사용자 경험 사이의 적절한 균형을 맞추기 위해 많은 단계가 필요하기 때문에 악명 높은 어려운 작업이다. 물론 그러한 AI 추론의 기본 목표은 더 적은 비용으로 더 많은 토큰을 생성하는 것이다. 토큰은 초거대 언어 모델(LLM) 시스템에서 단어를 나타내며, AI 추론 서비스가 일반적으로 생성된 백만 개의 토큰마다 요금을 부과하기 때문에 이러한 목표는 AI 투자와 사용된 에너지당 작업의 가장 눈에 띄는 수익을 제공한다. 

아울러, 엔비디아의 AI 추론 플랫폼 사용자를 위한 지속적인 소프트웨어 최적화와 이전 세대에 비해 추론 작업에 대해 최대 15배 더 높은 에너지 효율성을 제공하는 호퍼 GPU 플랫폼의 능력을 결합하여 이 목표를 달성했다. 또한, 추론의 경제적 이익을 얻은 엔비디아와 협력하는 여러 파트너와 고객의 예를 들자면 다음과 같다.

매월 4억 3,500만 건 이상의 질의를 처리하는 AI 기반 검색 엔진인 퍼플렉시티 AI는 다수의 AI 추론 요청을 하는 데, 이러한 다수의 요청을 관리하기 위해 엔비디아의 H100 GPU, 트리톤 추론 서버(Triton Inference Server), 및 TensorRT-LLM을 사용한다.

한편, 퍼플렉시티는 라마 3의 8B 및 70B 트랜스포머과 같은 20개 이상의 AI 모델을 지원하며, 검색, 요약, 질문 답변과 같은 다양한 작업을 처리한다. 작은 분류기 모델을 사용하여 작업을 GPU 팟으로 라우팅하고, 엔비디아 트리톤 서버가 관리하는 퍼플렉시티는 엄격한 서비스 수준 계약 하에서 비용 효율적이고 응답성이 뛰어난 서비스를 제공한다. 모델 병렬화를 통해 LLM을 GPU에 분할해 퍼플렉시티는 저지연 및 고정확도를 유지하면서 비용을 세 배로 절감했다.

## 엔비디아의 추론 비용 절감 및 효율성 향상의 비결

그렇다면, 최근에 엔비디아의 추론 비용 절감 및 효율성 향상의 비결은 무엇일까?

바로 애플에서 제공한 **추측 기반 디코딩(Speculative decoding)**의 오픈 소스 접근 방식인 [ReDrafter](https://github.com/apple/ml-recurrent-drafter)를 [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)에 통합한 것 때문이다. ReDrafter는 작은 **'드래프트(drafter)'** 모듈을 사용하여 토큰을 병렬로 예측한 다음, 주 모델에서 이를 검증하는 방식이다. 이 기법은 특히 저조한 트래픽 기간 동안 초거대 언어 모델의 응답 시간을 크게 줄이는 데 효과가 있다.

한편, **트리톤(Triton) 추론 서버**는 엔비디아의 고객인 도큐사인(Docusign)이 추론을 최적화하는 데 중요한 역할을 했다. 도큐사인은 더 이상 AI 모델을 위한 맞춤형, 프레임워크 특화 추론 서버를 배포할 필요가 없다. 모든 AI 프레임워크에 대해 트리톤을 통합된 추론 서버로 활용하고 있으며, 비용 및 성능 최적화를 위한 적절한 프로덕션 시나리오를 식별하는 데도 사용하고 있다.

결론적으로 말해서, 엔비디아의 블랙웰 GPU 및 소프트웨어 최적화 기술은 AI 추론 비용 절감과 효율성 향상을 통해 **“수익성 AI”**로의 길을 열며, 비용 절감과 성능 최적화를 달성하기 위해 엔비디아는 NIM 서비스가 탄생하게 되었다.