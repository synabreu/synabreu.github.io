---
title: "[실습] PDF 문서 기반 챗봇"
date: 2024-06-08
tags: [마이크로소프트, Microsoft, 오픈AI, OpenAI, FAISS, VectorDB, RAG, LangChain]
typora-root-url: ../
toc: true
categories: [Microsoft, OpenAI, FAISS, RAG, LangChain]
---



 

사용자가 문서를 업로드하면 문서 내용을 임베딩하고, 그 내용을 바탕으로 질문에 응답하는 간단한 PDF 문서 기반 챗봇 시스템을 만들어 보겠다. 조금 더 심층 학습을 진행해보겠다.



## 1. 현재 환경에 필수 컴포넌트 설치

```cmd
pip install streamlit dotenv langchain langchain_community pdfminer.six
```

* **streamlit 컴포넌트:** 대화형 대시보드나 LLM 챗봇 UI를 빠르게 만들 수 있는 Python 스크립트를 웹 앱처럼 실행할 수 있게 해주는 프레임워크
* **dotnev 컴포넌트:** `.env` 파일에 저장된 환경변수를 Python 코드에서 불러오기 위한 패키지
* **langchain 컴포넌트:** 문서 QA, 에이전트, 체인 구성, Tool 사용, 메모리 관리 등 LLM 기반 애플리케이션 구축을 위한 프레임워크
* **langchain_community 컴포넌트:** FAISS, Chroma, HuggingFace, OpenAI 등의 커넥터 및 벡터 DB 연동 등 LangChain 커뮤니티에서 관리하는 플러그인, 도구, 래퍼들을 포함한 확장 패키지.
* **pdfminer.six 컴포넌트:** PDF 문서에서 텍스트를 추출하는 데 사용한 패키지



## 2. 필수 라이브러리 import  추가 

```python
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import ChatMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from pdfminer.high_level import extract_text
```



## 3. 환경 설정 및 임베딩 초기화

```python
# 환경 변수 설정
load_dotenv()
os.environ["OPENAI_API_KEY"] = "<YOUR_OPENAI_KEY>"

# 임베딩 초기화 - 환경 변수 OPENAI_API_KEY를 자동으로 인식하여 OpenAI API에 접근함
embedding_model = OpenAIEmbeddings()
```

* load_dotenv(): `.env` 파일에 `OPENAI_API_KEY=sk-xxxx`가 있으면, 이 키가 시스템 환경 변수로 등록함
* embedding_model = OpenAIEmbeddings()
  * **기능**: OpenAI의 텍스트 임베딩 모델(기본적으로 `text-embedding-ada-002`)을 초기화함
  * **사용 목적**: 문장을 벡터(숫자 배열)로 변환해 유사도 검색, 벡터 DB 저장, 검색 기반 생성(RAG) 등에 사용됨



## 4. MarkdownStreamHandler 클래스 정의

```python
class MarkdownStreamHandler(BaseCallbackHandler):
    """
    Streamlit 마크다운 컨테이너에 생성된 토큰을 실시간으로 스트리밍하는 사용자 정의 핸들러.
    """
    def __init__(self, output_container, initial_content=""):
        self.output_container = output_container
        self.generated_content = initial_content

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.generated_content += token
        self.output_container.markdown(self.generated_content)
```

* 용도:  **LangChain과 Streamlit을 연동**하여 **LLM(Large Language Model)이 생성하는 텍스트를 마크다운 형태로 실시간 출력**하기 위한 **사용자 정의 콜백 핸들러 클래스**

* 주요 기능: LangChain의 `on_llm_new_token` 이벤트 처리

* 사용처: 실시간 챗봇 UI, LLM 인터페이스 구현

* `__init__` 생성자(constructor)

  * `output_container`: Streamlit의 출력 영역 (`st.empty()` 또는 `st.container()`)을 전달받아 저장함
  * `initial_content`: 초기 텍스트 내용을 지정할 수 있는 선택적 인자. 
  * 이 값은  LLM이 토큰을 출력하기 전의 상태

* on_llm_new_token 메서드

  * LLM이 새 토큰을 생성할 때마다 자동 호출되는 함수
  * `token`: 현재 생성된 단일 토큰 문자열
  * 동작
    * 기존 `generated_content` 문자열에 새로운 토큰을 붙임
    * 그 컨텐츠를 `markdown()`으로 출력 컨테이너에 업데이트함
    * 결과적으로 사용자는 Streamlit 앱에서 LLM이 생성하는 응답을 **실시간으로 확인**할 수 있게 됨

* 전체 흐름

  * 사용자 질문을 입력하면 LangChain LLM이 응답 생성을 시작함
  * MarkdownStreamHandle 핸들러는 LangChain에 `callbacks=[MarkdownStreamHandler(...)]` 식으로 전달됨
  * LLM이 응답 텍스트를 한 토큰씩 생성하면서 `on_llm_new_token()`이 반복 호출됨
  * 그때마다 `output_container.markdown(...)`으로 실시간 업데이트가 이루어짐

  

## 5. PDF 텍스트 추출 함수 추가

```python
def extract_text_from_pdf(file):
    """pdfminer를 사용하여 PDF 파일에서 텍스트 추출."""
    try:
        return extract_text(file)
    except Exception as error:
        st.error(f"PDF에서 텍스트 추출 중 오류 발생: {error}")
        return ""
```

* 함수 명:  extract_text_from_pdf
* 인자: `file` – PDF 파일 객체 또는 경로
* 역할: 입력된 PDF 파일로부터 텍스트를 추출
* docstring: 함수 설명 (한글로 작성됨)
* `extract_text`는 `pdfminer.high_level.extract_text` 함수로, PDF에서 텍스트를 추출함
* PDF 내 페이지들을 파싱해 텍스트만 반환하고, 성공 시 바로 추출된 텍스트를 return 함



