---
title: "[ì‹¤ìŠµ] PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡"
date: 2024-06-08
tags: [ë§ˆì´í¬ë¡œì†Œí”„íŠ¸, Microsoft, ì˜¤í”ˆAI, OpenAI, FAISS, VectorDB, RAG, LangChain]
typora-root-url: ../
toc: true
categories: [Microsoft, OpenAI, FAISS, RAG, LangChain]
---



 

ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ì„ë² ë”©í•˜ê³ , ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì‘ë‹µí•˜ëŠ” ê°„ë‹¨í•œ PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ ë³´ê² ë‹¤. ì¡°ê¸ˆ ë” ì‹¬ì¸µ í•™ìŠµì„ ì§„í–‰í•´ë³´ê² ë‹¤.



## 1. í˜„ì¬ í™˜ê²½ì— í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜

```python
pip install streamlit dotenv langchain langchain_community pdfminer.six
```

* **streamlit ì»´í¬ë„ŒíŠ¸:** ëŒ€í™”í˜• ëŒ€ì‹œë³´ë“œë‚˜ LLM ì±—ë´‡ UIë¥¼ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì›¹ ì•±ì²˜ëŸ¼ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬
* **dotnev ì»´í¬ë„ŒíŠ¸:** `.env` íŒŒì¼ì— ì €ì¥ëœ í™˜ê²½ë³€ìˆ˜ë¥¼ Python ì½”ë“œì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ íŒ¨í‚¤ì§€
* **langchain ì»´í¬ë„ŒíŠ¸:** ë¬¸ì„œ QA, ì—ì´ì „íŠ¸, ì²´ì¸ êµ¬ì„±, Tool ì‚¬ìš©, ë©”ëª¨ë¦¬ ê´€ë¦¬ ë“± LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬
* **langchain_community ì»´í¬ë„ŒíŠ¸:** FAISS, Chroma, HuggingFace, OpenAI ë“±ì˜ ì»¤ë„¥í„° ë° ë²¡í„° DB ì—°ë™ ë“± LangChain ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê´€ë¦¬í•˜ëŠ” í”ŒëŸ¬ê·¸ì¸, ë„êµ¬, ë˜í¼ë“¤ì„ í¬í•¨í•œ í™•ì¥ íŒ¨í‚¤ì§€.
* **pdfminer.six ì»´í¬ë„ŒíŠ¸:** PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©í•œ íŒ¨í‚¤ì§€



## 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import  ì¶”ê°€ 

```python
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import ChatMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from pdfminer.high_level import extract_text
```



## 3. í™˜ê²½ ì„¤ì • ë° ì„ë² ë”© ì´ˆê¸°í™”

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()
os.environ["OPENAI_API_KEY"] = "<YOUR_OPENAI_KEY>"

# ì„ë² ë”© ì´ˆê¸°í™” - í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ì—¬ OpenAI APIì— ì ‘ê·¼í•¨
embedding_model = OpenAIEmbeddings()
```

* load_dotenv(): `.env` íŒŒì¼ì— `OPENAI_API_KEY=sk-xxxx`ê°€ ìˆìœ¼ë©´, ì´ í‚¤ê°€ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•¨
* embedding_model = OpenAIEmbeddings()
  * **ê¸°ëŠ¥**: OpenAIì˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸(ê¸°ë³¸ì ìœ¼ë¡œ `text-embedding-ada-002`)ì„ ì´ˆê¸°í™”í•¨
  * **ì‚¬ìš© ëª©ì **: ë¬¸ì¥ì„ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•´ ìœ ì‚¬ë„ ê²€ìƒ‰, ë²¡í„° DB ì €ì¥, ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±(RAG) ë“±ì— ì‚¬ìš©ë¨



## 4. MarkdownStreamHandler í´ë˜ìŠ¤ ì •ì˜

```python
class MarkdownStreamHandler(BaseCallbackHandler):
    """
    Streamlit ë§ˆí¬ë‹¤ìš´ ì»¨í…Œì´ë„ˆì— ìƒì„±ëœ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¸ë“¤ëŸ¬.
    """
    def __init__(self, output_container, initial_content=""):
        self.output_container = output_container
        self.generated_content = initial_content

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.generated_content += token
        self.output_container.markdown(self.generated_content)
```

* ìš©ë„:  **LangChainê³¼ Streamlitì„ ì—°ë™**í•˜ì—¬ **LLM(Large Language Model)ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì‹¤ì‹œê°„ ì¶œë ¥**í•˜ê¸° ìœ„í•œ **ì‚¬ìš©ì ì •ì˜ ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤**

* ì£¼ìš” ê¸°ëŠ¥: LangChainì˜ `on_llm_new_token` ì´ë²¤íŠ¸ ì²˜ë¦¬

* ì‚¬ìš©ì²˜: ì‹¤ì‹œê°„ ì±—ë´‡ UI, LLM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

* `__init__` ìƒì„±ì(constructor)

  * `output_container`: Streamlitì˜ ì¶œë ¥ ì˜ì—­ (`st.empty()` ë˜ëŠ” `st.container()`)ì„ ì „ë‹¬ë°›ì•„ ì €ì¥í•¨
  * `initial_content`: ì´ˆê¸° í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì§€ì •í•  ìˆ˜ ìˆëŠ” ì„ íƒì  ì¸ì. 
  * ì´ ê°’ì€  LLMì´ í† í°ì„ ì¶œë ¥í•˜ê¸° ì „ì˜ ìƒíƒœ

* on_llm_new_token ë©”ì„œë“œ

  * LLMì´ ìƒˆ í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ ìë™ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
  * `token`: í˜„ì¬ ìƒì„±ëœ ë‹¨ì¼ í† í° ë¬¸ìì—´
  * ë™ì‘
    * ê¸°ì¡´ `generated_content` ë¬¸ìì—´ì— ìƒˆë¡œìš´ í† í°ì„ ë¶™ì„
    * ê·¸ ì»¨í…ì¸ ë¥¼ `markdown()`ìœ¼ë¡œ ì¶œë ¥ ì»¨í…Œì´ë„ˆì— ì—…ë°ì´íŠ¸í•¨
    * ê²°ê³¼ì ìœ¼ë¡œ ì‚¬ìš©ìëŠ” Streamlit ì•±ì—ì„œ LLMì´ ìƒì„±í•˜ëŠ” ì‘ë‹µì„ **ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸**í•  ìˆ˜ ìˆê²Œ ë¨

* ì „ì²´ íë¦„

  * ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ LangChain LLMì´ ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•¨
  * MarkdownStreamHandle í•¸ë“¤ëŸ¬ëŠ” LangChainì— `callbacks=[MarkdownStreamHandler(...)]` ì‹ìœ¼ë¡œ ì „ë‹¬ë¨
  * LLMì´ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í•œ í† í°ì”© ìƒì„±í•˜ë©´ì„œ `on_llm_new_token()`ì´ ë°˜ë³µ í˜¸ì¶œë¨
  * ê·¸ë•Œë§ˆë‹¤ `output_container.markdown(...)`ìœ¼ë¡œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì§

  

## 5. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ì¶”ê°€

```python
def extract_text_from_pdf(file):
    """pdfminerë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
    try:
        return extract_text(file)
    except Exception as error:
        st.error(f"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""
```

* í•¨ìˆ˜ ëª…:  extract_text_from_pdf
* ì¸ì: `file` â€“ PDF íŒŒì¼ ê°ì²´ ë˜ëŠ” ê²½ë¡œ
* ì—­í• : ì…ë ¥ëœ PDF íŒŒì¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
* docstring: í•¨ìˆ˜ ì„¤ëª… (í•œê¸€ë¡œ ì‘ì„±ë¨)
* `extract_text`ëŠ” `pdfminer.high_level.extract_text` í•¨ìˆ˜ë¡œ, PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•¨
* PDF ë‚´ í˜ì´ì§€ë“¤ì„ íŒŒì‹±í•´ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³ , ì„±ê³µ ì‹œ ë°”ë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ return í•¨



## 6. PDF ë¬¸ì„œ ì—…ë¡œë“œ í•¨ìˆ˜ ì¶”ê°€

```python
def handle_uploaded_file(file):
    """ì—…ë¡œë“œëœ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„."""
    if not file: # ì—…ë¡œë“œ ì—¬ë¶€ í™•ì¸
        return None, None

    # íŒŒì¼ ìœ í˜•ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    document_text = extract_text_from_pdf(file) if file.type == 'application/pdf' else ""
    if not document_text:
        st.error("ì—…ë¡œë“œëœ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    # ë¬¸ì„œë¥¼ ë¬¸ì„œ ë‹¨ë½(ì‘ì€ ì²­í¬)ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„°í™” ì¤€ë¹„
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200
    )
    document_chunks = text_splitter.create_documents([document_text])
    st.info(f"{len(document_chunks)}ê°œì˜ ë¬¸ì„œ ë‹¨ë½ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(document_chunks, embedding_model)
    return vectorstore, document_text
```

* ì „ì²´ ê³¼ì •

  * ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ PDF íŒŒì¼ì„ ì²˜ë¦¬í•¨
  * í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ ,
  * í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë’¤,
  * ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° DBì— ì €ì¥
  * LangChain ê¸°ë°˜ ë¬¸ì„œ ì„ë² ë”© ì²˜ë¦¬ í•¨ìˆ˜
  * ì‹¤ì‹œê°„ ì²˜ë¦¬ ê²°ê³¼ëŠ” Streamlitìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ í”¼ë“œë°±ì„ ì œê³µí•¨

* handle_uploaded_file í•¨ìˆ˜

  * ëª©ì : PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ë½‘ê³  ì²­í¬ë¡œ ë‚˜ëˆˆ ë’¤ FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë§Œë“¤ì–´ ê²€ìƒ‰ ì¤€ë¹„ë¥¼ ì™„ë£Œí•¨
    * ë°˜í™˜ê°’: `(vectorstore, document_text)`
    * `vectorstore`: FAISS ê¸°ë°˜ ë²¡í„° DB
    * `document_text`: ì „ì²´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸

* ì—…ë¡œë“œ ì—¬ë¶€ í™•ì¸: íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° â†’ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•˜ê³  `None` ë°˜í™˜.

* í…ìŠ¤íŠ¸ ì¶”ì¶œ: 

  * PDFì¸ ê²½ìš°ì—ë§Œ `extract_text_from_pdf()` í•¨ìˆ˜ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì¶”ì¶œ.
  * ê·¸ ì™¸ íŒŒì¼ í˜•ì‹ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŒ (ì¶”í›„ DOCX ë“± í™•ì¥ ê°€ëŠ¥).
  * ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì—ëŸ¬ í‘œì‹œ

* í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 

  * LangChainì˜ `CharacterTextSplitter`ë¥¼ ì‚¬ìš©í•´ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ”
  * íŒŒë¼ë¯¸í„°:
    * `chunk_size=1000`: ìµœëŒ€ 1,000ìì”© ë‚˜ëˆ”
    * `chunk_overlap=200`: ë‹¨ë½ ê°„ ì¤‘ë³µ 200ì í¬í•¨ (ë¬¸ë§¥ ìœ ì§€ìš©)

* ë²¡í„° ìŠ¤í† ì–´(Faiss) ìƒì„±

  * ìƒì„±ëœ í…ìŠ¤íŠ¸ ë‹¨ë½ë“¤ì„ `embedding_model`ë¡œ ì„ë² ë”©í•˜ê³ , **FAISS ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥**í•¨
  * ì´ ê°ì²´ëŠ” ì¶”í›„ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤ (ì˜ˆ: RAG ì±—ë´‡).

* return

  * `vectorstore`: ì§ˆì˜-ì‘ë‹µ, ê²€ìƒ‰ ë“±ì— í™œìš©
  * `document_text`: ì›ë¬¸ ì „ì²´, ë¡œê·¸ë‚˜ ìš”ì•½ ë“±ì— í™œìš© ê°€ëŠ¥

  

## 7. RAGë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì¶”ê°€

```python
def get_rag_response(user_query, vectorstore, callback_handler):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±."""
    if not vectorstore:
        st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return ""

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
    retrieved_docs = vectorstore.similarity_search(user_query, k=3)
    retrieved_text = "\n".join(f"ë¬¸ì„œ {i+1}: {doc.page_content}" for i, doc in enumerate(retrieved_docs))
    print(retrieved_text)

    # LLM ì„¤ì •
    chat_model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback_handler])

    # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
    rag_prompt = [
        SystemMessage(content="ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."),
        HumanMessage(content=f"ì§ˆë¬¸: {user_query}\n\n{retrieved_text}")
    ]

    try:
        response = chat_model(rag_prompt)
        return response.content
    except Exception as error:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""
```

* get_rag_response()
  * **RAG (Retrieval-Augmented Generation)** ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ” **LangChain ê¸°ë°˜ Q&A ì‹œìŠ¤í…œì˜ í•µì‹¬ ë¡œì§**
  * ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ëŒ€í•´, ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , í•´ë‹¹ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ GPT ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•¨
  * ëª©ì : ì‚¬ìš©ì ì§ˆë¬¸(`user_query`)ì„ ì…ë ¥ ë°›ì•„, ë²¡í„° ìŠ¤í† ì–´(`vectorstore`)ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„, LLMì—ê²Œ í•´ë‹¹ ë¬¸ì„œì™€ í•¨ê»˜ ì§ˆë¬¸ì„ ì „ë‹¬í•´ **ì •í™•í•˜ê³  ê·¼ê±° ìˆëŠ” ë‹µë³€ì„ ìƒì„±**í•¨
* ë²¡í„° ìŠ¤í† ì–´ ì²´í¬
  * ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•„ `vectorstore`ê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  í•¨ìˆ˜ ì¢…ë£Œ.
* ë¬¸ì„œ ê²€ìƒ‰
  * `vectorstore`ì—ì„œ **ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ 3ê°œì˜ ë¬¸ì„œ**ë¥¼ ê²€ìƒ‰í•¨
  * `similarity_search()`ëŠ” ë‚´ì¥ëœ ë¬¸ì¥ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°ì„ ìˆ˜í–‰í•¨
  * ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³‘í•©í•˜ì—¬ ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•¨
  * Streamlitì—ëŠ” ì¶œë ¥ë˜ì§€ ì•Šê³  ì½˜ì†”ì—ë§Œ ì¶œë ¥ë¨ (`print()`).
* gpt-4o-mini LLM ì„¤ì •
  * LangChainì˜ `ChatOpenAI` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ GPT-4o-mini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•¨
  * ì˜µì…˜
    * `temperature=0`: ê°€ì¥ í™•ì •ì ì¸(ì •í™•ë„ ë†’ì€) ì‘ë‹µì„ ìƒì„±
    * `streaming=True`: ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥ (UIì— ì ì§„ì  ì¶œë ¥)
    * `callbacks=[callback_handler]`: `MarkdownStreamHandler` ë“± ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì „ë‹¬
* í”„ë¡¬í”„íŠ¸ êµ¬ì„± (RAG ë°©ì‹)
  * `SystemMessage`: LLMì—ê²Œ ì—­í• ì„ ì§€ì •í•¨ ("ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µí•˜ë¼", "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ë¼")
  * `HumanMessage`: ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ + ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
* ì‘ë‹µ ìƒì„±
  * `chat_model()`ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ì‘ë‹µ ë‚´ìš©ì„ ë¦¬í„´í•¨

| í•­ëª©      | ì„¤ëª…                                                         |
| --------- | ------------------------------------------------------------ |
| ì…ë ¥ê°’    | `user_query` (ì§ˆë¬¸), `vectorstore` (ë¬¸ì„œ ë²¡í„° DB), `callback_handler` (í† í° ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬) |
| í•µì‹¬ ê¸°ëŠ¥ | ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ LLM í˜¸ì¶œ â†’ ì‘ë‹µ ìƒì„±        |
| ë°˜í™˜ê°’    | LLMì´ ìƒì„±í•œ ìµœì¢… ì‘ë‹µ ë¬¸ìì—´                                |
| ì‚¬ìš© ê¸°ìˆ  | LangChain, OpenAI GPT, FAISS ë²¡í„° ê²€ìƒ‰, Streamlit ì˜¤ë¥˜ ì¶œë ¥  |
| ëª©ì       | RAG ë°©ì‹ Q&A ì±—ë´‡ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ì„± ìˆëŠ” ì‘ë‹µ ì œê³µ |



## 8. Streamlit UI ì„¤ì •

```python
st.set_page_config(page_title="PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")
st.title("ğŸ“„ PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")
```

* ìš©ë„
  * **Streamlit ì•±ì˜ ì´ˆê¸° í˜ì´ì§€ ì„¤ì •ê³¼ íƒ€ì´í‹€ êµ¬ì„±**ì„ ë‹´ë‹¹í•¨
  * ì£¼ë¡œ ì›¹ í˜ì´ì§€ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³´ì´ê²Œ í•˜ê³ , ì£¼ì œë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•¨
* st.set_page_config(...)
  * **Streamlit ì•±ì˜ ë©”íƒ€ë°ì´í„°(í˜ì´ì§€ ì„¤ì •)ë¥¼ ì§€ì •**í•¨
  * ì£¼ìš” íŒŒë¼ë¯¸í„°:
    * page_title: ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œë  ì œëª© â†’ ì˜ˆ: ğŸ“„ PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡
    * ì´ ì™¸ì—ë„ `page_icon`, `layout`, `initial_sidebar_state` ë“±ì„ ì„¤ì •í•  ìˆ˜ ìˆìŒ.
* st.title(...)
  * Streamlit ì•± ìƒë‹¨ì— í¬ê²Œ **íƒ€ì´í‹€**ì„ í‘œì‹œí•¨



## 9. ì„¸ì…˜ ìƒíƒœì˜ ë³€ìˆ˜ ì´ˆê¸°í™”

```python
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = [
        ChatMessage(role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”? ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš©ì˜ ë²”ìœ„ì—ì„œë§Œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")
    ]
```



## 10. PDF íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬

```python
uploaded_file = st.file_uploader("PDF ë¬¸ì„œë§Œ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”!:", type=["pdf"])
if uploaded_file and uploaded_file != st.session_state.get("uploaded_file"):
    vectorstore, document_text = handle_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state["vectorstore"] = vectorstore
        st.session_state["uploaded_file"] = uploaded_file
```



## 11. ì±„íŒ… ê¸°ë¡ í‘œì‹œ

```python
chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_history:
        st.chat_message(message.role).write(message.content)
```



## 12. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥

```
if user_query := st.chat_input("ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:"):
    st.session_state.chat_history.append(ChatMessage(role = "user", content = user_query))
    with chat_container:
        st.chat_message("user").write(user_query)
```



## 13. ì‘ë‹µ ìƒì„±

```python
    with st.chat_message("assistant"):
        stream_output = MarkdownStreamHandler(st.empty())
        assistant_response = get_rag_response(user_query, st.session_state.get("vectorstore"), stream_output)
        if assistant_response:
            st.session_state.chat_history.append(ChatMessage(role="assistant", content = assistant_response))
```

