---
title: "[실습] PDF 문서 기반 챗봇"
date: 2024-06-08
tags: [마이크로소프트, Microsoft, 오픈AI, OpenAI, FAISS, VectorDB, RAG, LangChain]
typora-root-url: ../
toc: true
categories: [Microsoft, OpenAI, FAISS, RAG, LangChain]
---



 

사용자가 문서를 업로드하면 문서 내용을 임베딩하고, 그 내용을 바탕으로 질문에 응답하는 간단한 PDF 문서 기반 챗봇 시스템을 만들어 보겠다. 조금 더 심층 학습을 진행해보겠다.



## 1. 현재 환경에 필수 컴포넌트 설치

```python
pip install streamlit dotenv langchain langchain_community pdfminer.six
```

* **streamlit 컴포넌트:** 대화형 대시보드나 LLM 챗봇 UI를 빠르게 만들 수 있는 Python 스크립트를 웹 앱처럼 실행할 수 있게 해주는 프레임워크
* **dotnev 컴포넌트:** `.env` 파일에 저장된 환경변수를 Python 코드에서 불러오기 위한 패키지
* **langchain 컴포넌트:** 문서 QA, 에이전트, 체인 구성, Tool 사용, 메모리 관리 등 LLM 기반 애플리케이션 구축을 위한 프레임워크
* **langchain_community 컴포넌트:** FAISS, Chroma, HuggingFace, OpenAI 등의 커넥터 및 벡터 DB 연동 등 LangChain 커뮤니티에서 관리하는 플러그인, 도구, 래퍼들을 포함한 확장 패키지.
* **pdfminer.six 컴포넌트:** PDF 문서에서 텍스트를 추출하는 데 사용한 패키지



## 2. 필수 라이브러리 import  추가 

```python
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import ChatMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from pdfminer.high_level import extract_text
```



## 3. 환경 설정 및 임베딩 초기화

```python
# 환경 변수 설정
load_dotenv()
os.environ["OPENAI_API_KEY"] = "<YOUR_OPENAI_KEY>"

# 임베딩 초기화 - 환경 변수 OPENAI_API_KEY를 자동으로 인식하여 OpenAI API에 접근함
embedding_model = OpenAIEmbeddings()
```

* load_dotenv(): `.env` 파일에 `OPENAI_API_KEY=sk-xxxx`가 있으면, 이 키가 시스템 환경 변수로 등록함
* embedding_model = OpenAIEmbeddings()
  * **기능**: OpenAI의 텍스트 임베딩 모델(기본적으로 `text-embedding-ada-002`)을 초기화함
  * **사용 목적**: 문장을 벡터(숫자 배열)로 변환해 유사도 검색, 벡터 DB 저장, 검색 기반 생성(RAG) 등에 사용됨



## 4. MarkdownStreamHandler 클래스 정의

```python
class MarkdownStreamHandler(BaseCallbackHandler):
    """
    Streamlit 마크다운 컨테이너에 생성된 토큰을 실시간으로 스트리밍하는 사용자 정의 핸들러.
    """
    def __init__(self, output_container, initial_content=""):
        self.output_container = output_container
        self.generated_content = initial_content

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.generated_content += token
        self.output_container.markdown(self.generated_content)
```

* 용도:  **LangChain과 Streamlit을 연동**하여 **LLM(Large Language Model)이 생성하는 텍스트를 마크다운 형태로 실시간 출력**하기 위한 **사용자 정의 콜백 핸들러 클래스**

* 주요 기능: LangChain의 `on_llm_new_token` 이벤트 처리

* 사용처: 실시간 챗봇 UI, LLM 인터페이스 구현

* `__init__` 생성자(constructor)

  * `output_container`: Streamlit의 출력 영역 (`st.empty()` 또는 `st.container()`)을 전달받아 저장함
  * `initial_content`: 초기 텍스트 내용을 지정할 수 있는 선택적 인자. 
  * 이 값은  LLM이 토큰을 출력하기 전의 상태

* on_llm_new_token 메서드

  * LLM이 새 토큰을 생성할 때마다 자동 호출되는 함수
  * `token`: 현재 생성된 단일 토큰 문자열
  * 동작
    * 기존 `generated_content` 문자열에 새로운 토큰을 붙임
    * 그 컨텐츠를 `markdown()`으로 출력 컨테이너에 업데이트함
    * 결과적으로 사용자는 Streamlit 앱에서 LLM이 생성하는 응답을 **실시간으로 확인**할 수 있게 됨

* 전체 흐름

  * 사용자 질문을 입력하면 LangChain LLM이 응답 생성을 시작함
  * MarkdownStreamHandle 핸들러는 LangChain에 `callbacks=[MarkdownStreamHandler(...)]` 식으로 전달됨
  * LLM이 응답 텍스트를 한 토큰씩 생성하면서 `on_llm_new_token()`이 반복 호출됨
  * 그때마다 `output_container.markdown(...)`으로 실시간 업데이트가 이루어짐

  

## 5. PDF 텍스트 추출 함수 추가

```python
def extract_text_from_pdf(file):
    """pdfminer를 사용하여 PDF 파일에서 텍스트 추출."""
    try:
        return extract_text(file)
    except Exception as error:
        st.error(f"PDF에서 텍스트 추출 중 오류 발생: {error}")
        return ""
```

* 함수 명:  extract_text_from_pdf
* 인자: `file` – PDF 파일 객체 또는 경로
* 역할: 입력된 PDF 파일로부터 텍스트를 추출
* docstring: 함수 설명 (한글로 작성됨)
* `extract_text`는 `pdfminer.high_level.extract_text` 함수로, PDF에서 텍스트를 추출함
* PDF 내 페이지들을 파싱해 텍스트만 반환하고, 성공 시 바로 추출된 텍스트를 return 함



## 6. PDF 문서 업로드 함수 추가

```python
def handle_uploaded_file(file):
    """업로드된 PDF 파일을 처리하고 벡터 스토어 준비."""
    if not file: # 업로드 여부 확인
        return None, None

    # 파일 유형에 따라 텍스트 추출
    document_text = extract_text_from_pdf(file) if file.type == 'application/pdf' else ""
    if not document_text:
        st.error("업로드된 PDF 파일에서 텍스트를 추출할 수 없습니다.")
        return None, None

    # 문서를 문서 단락(작은 청크)로 나누어 벡터화 준비
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200
    )
    document_chunks = text_splitter.create_documents([document_text])
    st.info(f"{len(document_chunks)}개의 문서 단락이 생성되었습니다.")

    # 유사성 검색을 위한 벡터 스토어 생성
    vectorstore = FAISS.from_documents(document_chunks, embedding_model)
    return vectorstore, document_text
```

* 전체 과정

  * 사용자가 업로드한 PDF 파일을 처리함
  * 텍스트를 추출하고,
  * 텍스트를 청크 단위로 나눈 뒤,
  * 임베딩하여 FAISS 벡터 DB에 저장
  * LangChain 기반 문서 임베딩 처리 함수
  * 실시간 처리 결과는 Streamlit으로 사용자에게 피드백을 제공함

* handle_uploaded_file 함수

  * 목적: PDF 파일을 업로드하면 텍스트를 뽑고 청크로 나눈 뒤 FAISS 벡터 스토어를 만들어 검색 준비를 완료함
    * 반환값: `(vectorstore, document_text)`
    * `vectorstore`: FAISS 기반 벡터 DB
    * `document_text`: 전체 추출된 텍스트

* 업로드 여부 확인: 파일이 업로드되지 않은 경우 → 처리를 중단하고 `None` 반환.

* 텍스트 추출: 

  * PDF인 경우에만 `extract_text_from_pdf()` 함수를 통해 텍스트 추출.
  * 그 외 파일 형식은 현재 지원하지 않음 (추후 DOCX 등 확장 가능).
  * 추출 실패 시 다음과 같이 에러 표시

* 텍스트를 문서 단락으로 분할

  * LangChain의 `CharacterTextSplitter`를 사용해 긴 텍스트를 나눔
  * 파라미터:
    * `chunk_size=1000`: 최대 1,000자씩 나눔
    * `chunk_overlap=200`: 단락 간 중복 200자 포함 (문맥 유지용)

* 벡터 스토어(Faiss) 생성

  * 생성된 텍스트 단락들을 `embedding_model`로 임베딩하고, **FAISS 벡터 스토어에 저장**함
  * 이 객체는 추후 유사도 기반 검색에 사용됩니다 (예: RAG 챗봇).

* return

  * `vectorstore`: 질의-응답, 검색 등에 활용
  * `document_text`: 원문 전체, 로그나 요약 등에 활용 가능

  

## 7. RAG를 사용한 응답 생성 함수 추가

```python
def get_rag_response(user_query, vectorstore, callback_handler):
    """검색된 문서를 기반으로 사용자 질문에 대한 응답 생성."""
    if not vectorstore:
        st.error("벡터 스토어가 없습니다. 문서를 먼저 업로드하세요.")
        return ""

    # 가장 유사한 문서 3개 검색
    retrieved_docs = vectorstore.similarity_search(user_query, k=3)
    retrieved_text = "\n".join(f"문서 {i+1}: {doc.page_content}" for i, doc in enumerate(retrieved_docs))
    print(retrieved_text)

    # LLM 설정
    chat_model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback_handler])

    # RAG 프롬프트 생성
    rag_prompt = [
        SystemMessage(content="제공된 문서를 기반으로 사용자의 질문에 답변하세요. 정보가 없으면 '모르겠습니다'라고 답변하세요."),
        HumanMessage(content=f"질문: {user_query}\n\n{retrieved_text}")
    ]

    try:
        response = chat_model(rag_prompt)
        return response.content
    except Exception as error:
        st.error(f"응답 생성 중 오류 발생: {error}")
        return ""
```

* get_rag_response()
  * **RAG (Retrieval-Augmented Generation)** 방식으로 동작하는 **LangChain 기반 Q&A 시스템의 핵심 로직**
  * 사용자가 입력한 질문에 대해, 벡터 스토어에서 관련 문서를 검색하고, 해당 문서 내용을 바탕으로 GPT 모델이 답변을 생성함
  * 목적: 사용자 질문(`user_query`)을 입력 받아, 벡터 스토어(`vectorstore`)에서 관련 문서를 검색한 후, LLM에게 해당 문서와 함께 질문을 전달해 **정확하고 근거 있는 답변을 생성**함
* 벡터 스토어 체크
  * 문서가 업로드되지 않아 `vectorstore`가 없는 경우 에러 메시지를 출력하고 함수 종료.
* 문서 검색
  * `vectorstore`에서 **질문과 가장 유사한 3개의 문서**를 검색함
  * `similarity_search()`는 내장된 문장 임베딩을 기반으로 벡터 유사도 계산을 수행함
  * 검색된 문서 내용을 문자열로 병합하여 출력 형식으로 저장함
  * Streamlit에는 출력되지 않고 콘솔에만 출력됨 (`print()`).
* gpt-4o-mini LLM 설정
  * LangChain의 `ChatOpenAI` 클래스를 사용해 GPT-4o-mini 모델 인스턴스를 초기화함
  * 옵션
    * `temperature=0`: 가장 확정적인(정확도 높은) 응답을 생성
    * `streaming=True`: 실시간 토큰 스트리밍 가능 (UI에 점진적 출력)
    * `callbacks=[callback_handler]`: `MarkdownStreamHandler` 등 스트리밍 핸들러 전달
* 프롬프트 구성 (RAG 방식)
  * `SystemMessage`: LLM에게 역할을 지정함 ("문서를 기반으로만 답하라", "모르면 모른다고 말하라")
  * `HumanMessage`: 실제 사용자 질문 + 검색된 문서 내용
* 응답 생성
  * `chat_model()`에 프롬프트를 전달하여 응답을 생성하고, 응답 내용을 리턴함

| 항목      | 설명                                                         |
| --------- | ------------------------------------------------------------ |
| 입력값    | `user_query` (질문), `vectorstore` (문서 벡터 DB), `callback_handler` (토큰 스트리밍 핸들러) |
| 핵심 기능 | 관련 문서 검색 → 프롬프트 구성 → LLM 호출 → 응답 생성        |
| 반환값    | LLM이 생성한 최종 응답 문자열                                |
| 사용 기술 | LangChain, OpenAI GPT, FAISS 벡터 검색, Streamlit 오류 출력  |
| 목적      | RAG 방식 Q&A 챗봇에서 사용자 질문에 대해 문서를 기반으로 신뢰성 있는 응답 제공 |



## 8. Streamlit UI 설정

```python
st.set_page_config(page_title="PDF 문서 기반 챗봇")
st.title("📄 PDF 문서 기반 챗봇")
```

* 용도
  * **Streamlit 앱의 초기 페이지 설정과 타이틀 구성**을 담당함
  * 주로 웹 페이지를 사용자 친화적으로 보이게 하고, 주제를 명확히 전달하기 위해 사용함
* st.set_page_config(...)
  * **Streamlit 앱의 메타데이터(페이지 설정)를 지정**함
  * 주요 파라미터:
    * page_title: 브라우저 탭에 표시될 제목 → 예: 📄 PDF 문서 기반 챗봇
    * 이 외에도 `page_icon`, `layout`, `initial_sidebar_state` 등을 설정할 수 있음.
* st.title(...)
  * Streamlit 앱 상단에 크게 **타이틀**을 표시함



## 9. 세션 상태의 변수 초기화

```python
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = [
        ChatMessage(role="assistant", content="안녕하세요? 업로드된 문서 내용의 범위에서만 질문해 주세요.")
    ]
```



## 10. PDF 파일 업로드 처리

```python
uploaded_file = st.file_uploader("PDF 문서만 업로드해 주세요!:", type=["pdf"])
if uploaded_file and uploaded_file != st.session_state.get("uploaded_file"):
    vectorstore, document_text = handle_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state["vectorstore"] = vectorstore
        st.session_state["uploaded_file"] = uploaded_file
```



## 11. 채팅 기록 표시

```python
chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_history:
        st.chat_message(message.role).write(message.content)
```



## 12. 사용자 질문 입력

```
if user_query := st.chat_input("업로드된 문서를 기반으로 질문해 주세요:"):
    st.session_state.chat_history.append(ChatMessage(role = "user", content = user_query))
    with chat_container:
        st.chat_message("user").write(user_query)
```



## 13. 응답 생성

```python
    with st.chat_message("assistant"):
        stream_output = MarkdownStreamHandler(st.empty())
        assistant_response = get_rag_response(user_query, st.session_state.get("vectorstore"), stream_output)
        if assistant_response:
            st.session_state.chat_history.append(ChatMessage(role="assistant", content = assistant_response))
```

