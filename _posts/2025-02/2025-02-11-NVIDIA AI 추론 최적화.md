---
title: "NVIDIA 풀스택 솔루션으로 AI 추론 성능 최적화"
date: 2025-02-11
tags: [NVIDIA, 엔비디아, Triton, 추론, Inference, TensorRT-LLM, Multi-GPU, H100, Medusa, Speculative Decoding, Model Parellism, Model Parallelism, Mixed-Precision Training, Pruning, Quantization, Data Preprocessing]
typora-root-url: ../
toc: true
category: [NVIDIA]
---

AI 기반 애플리케이션의 폭발적인 증가로 인해, **개발자와 AI 인프라** 모두에게 전에 없던 수준의 부담이 가해지고 있다는 것을 누구나 잘 알 것이다. 더욱이 개발자는 **최첨단 성능을 제공하는 동시에, 운영 복잡성과 비용을 관리해야 하는 과제**를 안고 있다. 

따라서, NVIDIA는 **칩, 시스템, 소프트웨어에 걸친 풀스택 혁신**을 통해 개발자들을 지원하고 있으며, 이를 통해 AI 추론의 가능성을 새롭게 정의하고 있다. 이제 AI 추론은 그 어느 때보다 빠르고, 효율적이며, 확장 가능하게 진화하고 있는 AI 추론에 대해 NVIDIA 풀스택 솔루션을 알아보자. 



## 1. 고처리량, 저지연 AI 추론을 손쉽게 배포

* **NVIDIA Triton Inference Server 탄생**
  * 6년 전, NVIDIA는 **고처리량(high-throughput)** 및 **지연 민감(latency-critical)**한 **상용 AI 애플리케이션을 구축하는 개발자들을 위한 전용 AI 추론 서버**를 만들기 시작했음
  * 당시 많은 개발자들은 **프레임워크별로 따로 구성된 커스텀 추론 서버** 때문에 복잡성이 증가하고, 운영 비용이 상승하며, 지연 시간과 처리량에 대한 **엄격한 SLA(서비스 수준 계약)**를 충족하지 못하는 문제에 직면함.
  * **모든 AI 프레임워크의 모델을 서비스할 수 있는 오픈소스 플랫폼**으로,
     프레임워크별로 흩어져 있던 추론 서버를 하나로 통합하여 **AI 추론 배포를 단순화**하고 **추론 처리 용량을 향상**시켰음
  *  Triton을 **NVIDIA의 가장 널리 채택된 오픈소스 프로젝트 중 하나**로 만들었으며,
     현재는 **수백 개의 선도 기업들이 AI 모델을 효율적으로 상용 환경에 배포하는 데** Triton을 활용하고 있음
* **NVIDIA TensorRT**
  * 강력한 성능의 딥러닝 추론 라이브러리로, 개발자가 **정밀한 최적화를 가능하게 하는 API**를 통해 **커스터마이징된 고성능 추론**을 구현할 수 있게 해줌
* **NVIDIA NIM 마이크로서비스(NVIDIA NIM microservices)**
  * AI 모델을 **클라우드, 데이터센터, 워크스테이션 등 어디에서나 유연하게 배포할 수 있는 프레임워크**를 제공함
* Triton Inference Server, TensorRT, NIM 과 같은 솔루션을 통해 개발자들은 **다양한 환경에서 추론 성능을 극대화하며 AI 서비스를 효율적으로 운영**할 수 있음



## 2. AI  추론 워크로드 최적화

* AI 추론은 **풀스택 문제(full-stack problem)**이며, **고성능 인프라**와 이를 효율적으로 활용할 수 있는 **소프트웨어 최적화**가 필요함.
  * 원인: 더욱 더 복잡해지는 AI 추론 워크로드:  모델 크기는 계속 커지고, 지연 시간 제약은 더 엄격해지며, **AI 서비스를 활용하는 사용자 수 또한 급증**하고 있음
  * 해결책: 모델 성능을 향상시키기 위해 추론 단계에서도 더 많은 연산 자원이 사용하는 추론 시간 스케일링(test-time scaling)로 해결책.
* 동일한 하드웨어 플랫폼 위에서도 **AI 추론 성능을 지속적으로 향상시키는 것이 매우 중요함** -> 검증된 기술과 최신 추론 기술을 결합해 **속도, 확장성, 비용 효율성에서 놀라운 성과**를 달성할 수 있음
  * 모델 병렬화(Model Parallelism)
  * 혼합 정밀도 학습(Mixed-Precision Training)
  * 프루닝(Pruning)
  * 양자화(Quantization)
  * 데이터 전처리 최적화(Data Preprocessing Optimization)
* NVIDIA의 **TensorRT-LLM 라이브러리**는 대규모 언어 모델(LLM)의 추론 성능을 가속화하기 위해 최첨단 기능들을 다수 포함하고 있으며, 주요 기능은 다음과 같음
  * Prefill 과 KV 캐시 최적화
  * 디코딩 최적화
  * 멀티 GPU 추론
  * 양자화 및 저정밀 연산(Quantization and Lower-Precision Compute)



## 3. Prefill 및 KV 캐시 최적화

* [**KV 캐시 조기 재사용(Key-Value Cache Early Reuse)**](https://developer.nvidia.com/blog/5x-faster-time-to-first-token-with-nvidia-tensorrt-llm-kv-cache-early-reuse/)

  * 시스템 프롬프트를 여러 사용자 간에 **공유·재사용**함으로써, **첫 번째 토큰 생성 시간(TTFT, Time-To-First-Token)**을 최대 **5배까지 가속**함.
  * 유연한 KV 블록 크기 설정 및 효율적인 제거(eviction) 정책을 통해 **메모리 관리**를 원활하게 유지하면서, 다중 사용자 환경에서도 **빠른 응답 속도**를 실현함. 

* [**청크 기반 프리필(Chunked Prefill)**](https://developer.nvidia.com/blog/streamlining-ai-inference-performance-and-deployment-with-nvidia-tensorrt-llm-chunked-prefill/)

  * 프리필(prefill) 단계를 더 작은 작업으로 나누어 GPU 활용률을 높이고 **지연 시간을 줄이는 스마트한 배포 전략**
  * **사용자 요청이 변동되는 상황에서도 일관된 성능**을 보장하며, **추론 파이프라인을 간소화**함

* [**다중 턴 상호작용 가속화**](https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/)

  * NVIDIA GH200 슈퍼칩 아키텍처는 **KV 캐시를 효율적으로 오프로딩(offloading)**하여, **Llama 모델 기반의 멀티턴 대화형 응답**에서 TTFT를 **최대 2배 향상**시키고, 동시에 **높은 처리량**을 유지함

    

## 4. 디코딩 최적화

* [**장문 입력을 위한 멀티블록 어텐션(Multiblock Attention)**](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/)
  * 긴 입력 시퀀스를 처리하기 위한 방식으로, **TensorRT-LLM의 멀티블록 어텐션 기능**은 작업을 GPU의 **스트리밍 멀티프로세서(SM)**에 효율적으로 분산하여 **GPU 활용률을 극대화**함
  * 이를 통해 시스템 **처리량이 3배 이상 향상**되며, **하드웨어 추가 없이도 더 긴 컨텍스트 길이를 지원**할 수 있음
* [**추론 처리량 가속을 위한 추측 디코딩(Speculative Decoding)**](https://developer.nvidia.com/blog/tensorrt-llm-speculative-decoding-boosts-inference-throughput-by-up-to-3-6x/)
  * 작은 **드래프트(draft) 모델**과 큰 **타깃(target) 모델**을 함께 사용하는 방식으로,**최대 3.6배 빠른 추론 처리량**을 달성함.
  *  **빠르고 정확한 결과 생성을 보장**하며, **대규모 AI 응용에 적합한 워크플로우**를 실현함.
* [**Medusa 기반 추측 디코딩**](https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/)
  * **TensorRT-LLM 최적화에 포함된 Medusa 알고리즘**은, 다수의 다음 토큰을 **한 번에 예측**하여 NVIDIA HGX H200 플랫폼에서 **Llama 3.1 모델의 처리량을 최대 1.9배 향상**시킴. 
  * 고객 지원과 콘텐츠 생성과 같은 **고속 응답이 필요한 LLM 기반 애플리케이션**에 적합함.



## 5. 멀티 GPU 추론

* [**MultiShot 통신 프로토콜**](https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/)
  * 기존의 **Ring AllReduce** 연산은 멀티 GPU 환경에서 병목 현상을 유발할 수 있음. 
  *  **NVSwitch 기반의 TensorRT-LLM MultiShot** 프로토콜을 도입해, **GPU 수와 무관하게 통신 단계를 단 두 번으로 줄여** AllReduce 속도를 최대 **3배까지 향상**시켰음
  * 이를 통해 **저지연 추론을 효율적으로 확장**할 수 있게 되었음
* [**고동시성 효율성을 위한 파이프라인 병렬화(Pipeline Parallelism)**](https://developer.nvidia.com/blog/boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/)
  * 병렬화 기술은 **GPU 간의 빠르고 효율적인 데이터 전송**을 필요로 하며, 이를 위해 강력한 GPU 간 인터커넥트가 필수임 
  * NVIDIA H200 Tensor Core GPU에서의 파이프라인 병렬화는 **Llama 3.1 405B 모델에 대해 1.5배의 처리량 향상**, **Llama 2 70B 모델에 대해 1.2배 속도 향상**을 달성하며,**MLCommons의 MLPerf Inference 벤치마크**에서 범용성을 입증했음 

* [**대규모 NVLink 도메인**](https://developer.nvidia.com/blog/low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance/)
  * **NVIDIA GH200 NVL32 시스템**은 32개의 GH200 Grace Hopper 슈퍼칩을**NVLink Switch 시스템으로 연결**한 아키텍처
  * **TensorRT-LLM 최적화와 함께 Llama 모델의 TTFT(Time-To-First-Token)를 최대 3배까지 향상**시킴
  * **127 페타플롭스의 AI 연산 성능**을 제공하며,**실시간 AI 응답성의 새로운 기준을 제시**함



## 6. 양자화 및 저정밀 연산(Quantization and Lower-Precision Compute)

* [**정밀도와 성능을 위한 TensorRT 모델 옵티마이저**](https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/)
  * NVIDIA의 **맞춤형 FP8 양자화 기법**은 정확도를 유지하면서도 **최대 1.44배 높은 처리량**을 제공함. 
  * 지연 시간과 하드웨어 요구사항을 줄여, **비용 효율적인 AI 추론 배포**를 가능하게 함
* [**엔드 투 엔드 풀스택 최적화**](https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/)
  * *TensorRT 라이브러리**와 **FP8 Tensor Core**의 혁신은 **데이터센터용 GPU부터 엣지 장비까지** 다양한 장치에서 고성능을 실현함.
  * **Llama 3.2 모델 시리즈**는 NVIDIA 소프트웨어 스택으로 최적화되어 다양한 AI 배포 환경에서도 **유연하고 효율적인 성능을 발휘**함
* Triton 및 TensorRT-LLM의 다양한 기능들을 통해, 개발자들은 **더 빠르고 효율적인 LLM을 손쉽게 배포**할 수 있게 되었음 -> **더 많은 사용자 수요와 복잡한 태스크를 처리하는 AI 서비스가 가능**해졌음
* 기업이 **고객 응대 개선, 복잡한 프로세스 자동화, 데이터 기반 통찰 확보**에 있어
   새로운 기회를 창출할 수 있도록 도와줌



## 7. AI 추론 성능 평가

* **최고 수준의 AI 추론 성능을 제공하기 위해서는 칩, 시스템, 소프트웨어로 구성된 완전한 기술 스택**이 필요함 -> 처리량을 높이고, 토큰당 에너지 소비를 줄이며, 비용을 최소화할 수 있음

* **MLPerf Inference**는 AI 추론 성능을 측정하는 대표적인 산업 표준 벤치마크

  * 표준화된 조건에서의 추론 처리량을 측정하며, **광범위한 동료 검토(peer review)**를 거침
  * 정기적으로 업데이트되어 **최신 AI 기술을 반영**, 플랫폼 성능 평가에 신뢰할 수 있는 지표가 됨

* 최신 MLPerf Inference 결과

  * **NVIDIA Blackwell GPU**는 최근 MLPerf Inference에 처음 등장하여,**Llama 2 70B 테스트에서 NVIDIA H100 대비 최대 4배 더 높은 성능**을 기록했음
    * **2세대 트랜스포머 엔진** (FP4 Tensor Cores 포함)
    * **초고속 HBM3e GPU 메모리** (GPU당 대역폭 8TB/s)
  * NVIDIA의 소프트웨어 스택(TensorRT-LLM 등)도 Blackwell의 **FP4 연산 지원 기능**을 활용하도록 재설계되었고, 정확도 기준도 여전히 충족했음
  * **NVIDIA H200 Tensor Core GPU**는 데이터센터 부문 모든 벤치마크에서 우수한 성과를 보였음 -> Mixtral 8x7B 전문가 혼합(MoE) LLM, Llama 2 70B 모델, Stable Diffusion XL 텍스트-이미지 생성
  * **Hopper 아키텍처**는 지속적인 소프트웨어 최적화를 통해 **이전 라운드 대비 최대 27% 더 높은 추론 성능**을 기록했음

* NVIDIA Triton의 상용성과 성능의 동시 달성 

  * NVIDIA Triton Inference Server가 **H200 GPU 8개 기반 시스템에서 실행**된 결과,
     **Llama 2 70B 테스트에서 NVIDIA의 베어메탈 제출과 거의 동일한 성능**을 달성했음
  * **기능이 풍부한 상용 AI 추론 서버**와 **최고 성능** 사이에서 **더 이상 선택을 강요받지 않아도 된다는 것을 의미**

  

## 8. 참고 

* NVIDIA 블로그: https://developer.nvidia.com/blog/optimize-ai-inference-performance-with-nvidia-full-stack-solutions/
