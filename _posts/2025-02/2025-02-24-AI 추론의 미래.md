---
title: "AI 추론의 미래"
date: 2025-02-24
tags: [NVIDIA, 엔비디아, Triton, 추론, Inference, TensorRT-LLM, Multi-GPU, H100, Medusa, Speculative Decoding, Model Parellism, Model Parallelism, Mixed-Precision Training, Pruning, Quantization, Data Preprocessing]
typora-root-url: ../
toc: true
category: [NVIDIA]
---

AI 추론 환경은 혁신적인 기술 발전과 새로운 패러다임의 등장에 힘입어 빠르게 진화하고 있다. 데이터센터급 연산 자원의 확대로 사전 훈련(pretraining)이 가능한 모델의 크기가 점점 더 커지면서, 모델은 계속해서 더 똑똑해지고 있다.

**GPT-MoE 1.8T**와 같은 **희소 Mixture-of-Experts(MoE)** 아키텍처는 **모델 지능을 향상**시키는 동시에 **연산 효율성을 높이는 역할**을 하게 된다. 이러한 **대규모 모델(조밀하거나 희소하거나)**을 효과적으로 실행하려면, **각 GPU의 성능 자체도 비약적으로 향상**되어야 한다. 그렇다면, AI 추론의 미래는 어떨까? 떠오르는 트렌드와 신기술 측면에서 한번 요점 정리해 보았다. 



## 1. 차세대 생성형 AI용 NVIDIA Blackwell 아키텍처

* **NVIDIA Blackwell GPU 아키텍처**는 차세대 생성형 AI 추론을 위한 핵심 엔진
* 각 Blackwell GPU는 2세대 트랜스포머 엔진(Transformer Engine), 5세대 Tensor Core, FP4 저정밀 데이터 포맷 등을 탑재하고 있음
* FP4는 **연산 처리량을 증가시키고 메모리 요구사항을 줄이는** 데 효과적이며, 정확도를 유지하면서도 성능을 극대화하려면 **정교한 소프트웨어 최적화 기술**이 필요



## 2. 대규모 추론을 위합 집단 GPU 활용

* 고성능 모델의 **실시간 응답성을 확보하기 위해**, 가장 강력한 GPU들이 **집단으로 협력**해야 함
* [**NVIDIA GB200 NVL72 랙스케일 솔루션은**](https://developer.nvidia.com/blog/nvidia-gb200-nvl72-delivers-trillion-parameter-llm-training-and-real-time-inference/) 72개의 GPU를 NVLink로 연결해 하나의 거대한 GPU처럼 작동**하는 구조
* GPT-MoE 1.8T의 **실시간 추론 성능은 이전 세대 Hopper GPU 대비 최대 30배 향상**됨



## 3. 새로운 스케일링 법칙: 테스트 타임 컴퓨트(Test-Time Compute)

* OpenAI의 **o1 모델**로 처음 소개된 **테스트 타임 스케일링**은 **응답 품질과 정확도를 향상시키는 새로운 패러다임**
* 모델이 **최종 결과를 생성하기 전에 중간 토큰을 다수 생성하며 ‘사고(thinking)’하는 구조**
* 이 **추론 중심의 사고 모델(reasoning model)**은**복잡한 수학 문제 해결, 소스 코드 생성** 등의 분야에서 효과적
* **추론 시점에서의 연산 성능**이 AI 발전의 핵심 요소로 부상



## 4. 범용 인공지능(AGI)로 가는 길

* 데이터센터의 연산 성능 향상
* 사전 훈련, 사후 훈련, 테스트 타임 추론 등 **세 가지 스케일링 방식**
* **정교하게 조율된 소프트웨어와 인프라**에 달려 있음
* **매년 혁신 주기를 가속화**하고 있으며, 이를 통해 **AI 생태계가 한계를 지속적으로 돌파**할 수 있도록 플랫폼을 빠르게 발전



## 5. AI 추론 시작하기

* [**AI 추론을 시작하는 방법**에 대한 가이드](https://www.nvidia.com/en-us/lp/ai-data-science/how-to-get-started-with-ai-inference-series/)

* [**NVIDIA AI Inference 플랫폼**](https://www.nvidia.com/en-us/solutions/ai/inference/)

* [MLPerf Inference 를 통해 최신 추론 성능 업데이트를 지속적으로 확인](https://developer.nvidia.com/deep-learning-performance-training-inference/ai-inference)

* 데모 - [**NVIDIA NIM 마이크로서비스**를 빠르게 배포](https://www.youtube.com/watch?v=087spL8hMvM)

* [Generative AI를 NVIDIA NIM으로 쉽게 배포하는 가이드](https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/)
  * TensorRT, TensorRT-LLM, TensorRT Model Optimizer의 최적화 기능들이**NVIDIA NIM 마이크로서비스 기반의 상용 환경 배포**

* 2025년 3월 18일 부터 *NVIDIA Triton Inference Server 는  [NVIDIA Dynamo](https://developer.nvidia.com/dynamo)* 로 변경되었음

  