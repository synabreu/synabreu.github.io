---
title: "[실습] Slurm 설치 및 구성"
date: 2024-03-02
tags: [Linux, Slurm, HPC, distributed workload manager, Job Scheduler]
typora-root-url: ../
toc: true
categories: [Slurm, OpenSource]
---

지난 번 슬럼(Slurm)  노트에서 슬럼이 무엇인지, 어떻게 구성되는 지 간단하게 알아보았다. 그렇다면, 이번에는 Ubuntu 22.04에서 **Slurm Workload Manager**를 설치하고 구성하는 방법에 대해 정리해보겠다. 



## 1. 전제 조건

* Ubuntu 22.04 서버 (Master + Worker 노드들)
* sudo 권한
* 모든 노드는 **동일한 사용자 이름 및 UID**를 가져야 함
* 모든 노드가 **시간 동기화 (NTP)** 되어 있어야 함
* 모든 노드 간 **SSH 접속**이 가능해야 함



## 2.  Slurm 설치를 위한 기본 패키지 설치

```bash
sudo apt update
sudo apt install -y munge libmunge-dev libmunge2 \
    build-essential \
    curl vim wget \
    libpam0g-dev libmariadb-dev libmysqlclient-dev \
    mysql-client \
    mariadb-server \
    python3 \
    git \
    rpm
```



## 3.  MUNGE 설정 (모든 노드에서 진행)

```bash
# MUNGE 키 생성 (마스터 노드에서 1회)
sudo /usr/sbin/create-munge-key

# 키 권한 설정
sudo chown munge: /etc/munge/munge.key
sudo chmod 400 /etc/munge/munge.key

# 워커 노드들로 키 복사 (예: 192.168.1.101이 worker 노드 IP)
scp /etc/munge/munge.key user@192.168.1.101:/tmp
ssh user@192.168.1.101 "sudo mv /tmp/munge.key /etc/munge && sudo chown munge: /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
```



## 4.  MUNGE 데몬 시작 및 상태 확인

```bash
sudo systemctl enable munge
sudo systemctl start munge
sudo systemctl status munge
```



## 5.  Slurm 소스 다운로드 및 빌드

```bash
cd /usr/local/src
sudo wget https://download.schedmd.com/slurm/slurm-23.02.6.tar.bz2
sudo tar -xvjf slurm-23.02.6.tar.bz2
cd slurm-23.02.6

sudo ./configure
sudo make
sudo make install
```



## 6. Slurm 사용자 및 디렉터리 생성 (모든 노드)

```bash
sudo useradd -m slurm
sudo mkdir -p /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
sudo chown slurm: /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
```



## 7. Slurm 사용자 및 디렉터리 생성 (모든 노드)

* [Slurm 공식 웹 사이트의 `slurm.conf` 생성기 사용](https://slurm.schedmd.com/configurator.html)

  * 예시: `/etc/slurm/slurm.conf` 생성 (마스터 노드)

    ```bash
    sudo mkdir /etc/slurm
    sudo vim /etc/slurm/slurm.conf
    ```

```ini
# 예시
ClusterName=ubuntu-cluster
ControlMachine=master
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
SchedulerType=sched/backfill
NodeName=node[1-2] CPUs=4 State=UNKNOWN
PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP
```



## 8. 슬러럼 데몬 서비스 등록

* slurmctld (컨트롤러 노드에만)

  ```bash
  sudo tee /etc/systemd/system/slurmctld.service > /dev/null <<EOF
  [Unit]
  Description=Slurm controller daemon
  After=munge.service
  [Service]
  Type=simple
  User=slurm
  ExecStart=/usr/local/sbin/slurmctld -D
  [Install]
  WantedBy=multi-user.target
  EOF
  ```

* slurmd (모든 워커 노드)

  ```bash
  sudo tee /etc/systemd/system/slurmd.service > /dev/null <<EOF
  [Unit]
  Description=Slurm node daemon
  After=munge.service
  [Service]
  Type=simple
  User=root
  ExecStart=/usr/local/sbin/slurmd -D
  [Install]
  WantedBy=multi-user.target
  EOF
  ```

  

## 9. 서비스 시작 및 상태 확인

```bash
# 마스터에서
sudo systemctl enable slurmctld
sudo systemctl start slurmctld

# 워커에서
sudo systemctl enable slurmd
sudo systemctl start slurmd

# 상태 확인
sinfo
```



## 10. Slurm 테스트

```bash
srun -N1 -n1 hostname
```

* 정상적으로 워커 노드의 호스트명이 출력되면 클러스터 구성이 완료된 것



## 11. 추가 팁

* 방화벽이 있다면 포트 `6817`, `6818` 을 열어라
* 각 노드에서 `/etc/hosts` 파일에 마스터/노드들의 IP와 호스트명 매핑을 넣어라
