---
title: "허깅페이스 스페이스의 초대규모 훈련 가이드"
date: 2025-07-06
tags: [Hugginf Face, ]
typora-root-url: ../
toc: true
categories: [Hugging Face]
---

허깅페이스 스페이스의 초대규모 훈련 가이드 플레이북은 대규모 언어 모델을 수천 개의 GPU에 걸쳐 **효율적이고 확장 가능하게 훈련**하기 위한 종합적인 가이드이다. 이 플레이북은 **분산 훈련 기법의 전반적인 프레임워크와 각 기법의 trade-off**, 그리고 이를 실제 GPU 클러스터 환경에서 효과적으로 결합·실험하여 얻은 실질적인 벤치마크를 제공한다. 특히, 코드 예제와 인터랙티브한 시각 자료를 통해 개념적 이해뿐 아니라 실무 적용에도 유용하도록 설계되어 있다. 

익히 저는 이 노트에서 대규모 학습에서의 분산 훈련과 추론에 대해 이론과 실용적으로 정리해왔다. 이 플레이북에서는 그러한 분산 훈련과 추론 전략에 대해 핵심적인 내용을 노트로 정리해보겠다. 



## 1. 목표 및 동기

* LLM 훈련을 **단일 GPU → 수십/수백/수천 GPU** 스케일로 확장하는 데 필요한 지식과 코드 기법을 일목요연하게 정리한 오픈소스 플레이북
* 기존에 분산 훈련 관련 지식은 논문이나 흩어진 코드베이스에 산재되어 있었지만, 이를 하나의 흐름으로 통합하고 실증적인 벤치마크와 코드 예제를 통해 제공하는 것



## 2. 주요 분산 훈련 전략

* **데이터 병렬 (Data Parallelism)**
  * 각 GPU가 서로 다른 데이터 샘플을 처리
  * 장점: 구현이 간단하고 확장성이 좋음. 통신 오버헤드를 최소화하고 성능을 잘 유지.
  * 단점: 모델 크기 증가 시 메모리 한계에 부딪힘
* **텐서 병렬 (Tensor Parallelism)**
  * 단일 연산을 멀티 GPU에 분산하여 병렬 처리. 대형 매트릭스(sharding) 연산을 분할 처리. 모델 크기 증가 시 효과적이지만, 통신 및 구현 복잡성이 증가
  * 장점: 모델을 더 큰 규모로 확장 가능
  * 단점: GPU 간 통신이 많고 구현 복잡도 증가
* **파이프라인 병렬 (Pipeline Parallelism)**
  * 델의 레이어 단위로 분할하여 GPU에 순차적으로 할당. 파이프라인 버블 현상(처리 공백)을 줄이는 스케줄링 기법이 중요. 모델 레이어를 나누어 GPU에 순차적으로 배치
  * 장점: 메모리 절약 가능
  * 단점: 파이프라인 “버블”로 인한 비효율 존재
* **컨텍스트 병렬 (Context Parallelism)**
  * 매우 긴 입력 시퀀스를 멀티 GPU에 나누어 병렬 처리. 최대 128k+ 토큰과 같은 긴 문맥 처리에 유리하며 통신-계산 오버랩 기법을 활용
  * 장점: 128k 이상의 시퀀스도 처리 가능
  * 단점: 통신-계산 병목을 줄이기 위한 고급 기법 필요
* **전문가 병렬 (Expert Parallelism / MoE)**
  * Mixture of Experts 구조에서 각 expert를 GPU를 분산하여 처리. 토큰‑expert 간 all-to-all 통신을 수행하며, 대규모 MoE 모델에 적합
  * 장점: 파라미터 수는 많지만 계산은 적어 효율적
  * 단점: All-to-All 통신 필요



## 3. 효율과 평가: 실험 기반 최적화

* 512 GPU까지 포함한 **4,100회 이상의 실험**을 통해 다양한 모델과 분산 레이아웃을 벤치마킹
* **메모리, 계산 효율(MFU), 통신 병목**이라는 세 가지 관점에서 각 병렬화 기법의 장단점과 적합한 활용 시점을 제시
* 총 **4,100+ 실험**을 통해 각 병렬 기법의 메모리 효율, 계산 성능, 통신 병목 등을 평가
* 어떤 시나리오에서 어떤 병렬화를 선택해야 할지에 대한 구체적 가이드 제공
* 예:
  * 데이터 병렬은 구현이 쉽고 통신이 적지만, 메모리 이득이 없고 규모를 키울 경우 병목이 생김.
  * 중간 크기 모델: ZeRO + 데이터 병렬. ZeRO‑3, 텐서+파이프라인 병렬 등이 복합적으로 결합되어야 대형 모델의 메모리와 성능 요구를 충족할 수 있음 
  * 초대형 모델: 텐서 + 파이프라인 병렬 + 컨텍스트 병렬 조합
  * MoE 모델: Expert 병렬화 필수



## 4. 코드 구현

* **Nanotron 리포지토리**에서는 `3D 병렬(DP+TP+PP)`, ZeRO‑1 최적화, MoE, 파이프라인 1F1B 스케줄, CUDA 기반 타이밍 측정 등 다양한 병렬 훈련 구성 예제
  * 예제 디렉터리: `custom-dataloader`, `moe`, `mamba`, `spectral µTransfer` 등.
  * 벤치마크 데이터, 설정 파일, 로그도 함께 제공
* **Gradient checkpointing(activations 재계산)** 등을 포함한 데이터 병렬 구현 방식이 detailed하게 설명되어 있으며, 이 방법은 메모리를 줄이고 계산량은 일부 증가시키는 트레이드오프 구조



## 5. 코드 구현

* **데이터 병렬(DP)**: 그래디언트 통신을 backward 단계 중에 겹쳐 실행해 GPU 유휴 시간을 줄이는 고도화된 병렬화 전략 포함
* **ZeRO‑2/ZeRO‑3**:
  * ZeRO‑2는 그래디언트만 분할(reduce‑scatter),
  * ZeRO‑3는 파라미터까지 분할하며, on‑demand all‑gather 방식으로 메모리 절약과 통신 효율을 달성
  * 복잡한 통신–계산 겹침 전략 설명과 함께 `2Ψ + (2Ψ + kΨ)/Nd` 수식으로 메모리 절감 방식 해설
* **컨텍스트 병렬(Context Parallelism)**:
  * 긴 시퀀스(예: 8B 모델, 128k 토큰)에 대한 메모리 사용량 그래프와 파이프라인 병렬과의 조합 효율성을 보여줌
  * 1-forward-1-backward (1F1B) 스케줄과 FP8 혼합 정밀도 전략 도표 수록
* **인터랙티브 벤치마크 시각화**:
  * CPU/GPU 클러스터 512개 이상까지 실험된 4,000+ 스케일링 실험 기반 plot 제공



## 6. 결론

* **모델 확대 스케일**: 512 GPU 이상 시 다양한 병렬 기법의 결합 필수
* **시퀀스 길이가 매우 길 경우**: Context Parallelism 우선 활용
* **MoE 구조 채택 시**: Expert Parallelism 추가
* 벤치마크 표 및 안내 다이아그램을 통해 “언제, 어떤 기법을 써야 할지” 직관적으로 설명



## 7. 참고 자료

* [Ultra-scale playbook](https://nanotron-ultrascale-playbook.static.hf.space/index.html)

* [Josherich's Blog](https://www.josherich.me/podcast/gpu-mode/livestream-the-ultra-scale-playbook)

* [nanotron/ultrascale-playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook/discussions/57/files)

* [deepnews](https://deepnewz.com/ai-modeling/hugging-face-releases-150-page-ultra-scale-playbook-covering-5d-parallelism-zero-98f75458)

* [Lecture 48: The Ultra Scale Playbook](https://www.youtube.com/watch?v=1E8GDR8QXKw)

  

  